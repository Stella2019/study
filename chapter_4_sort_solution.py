# -*- coding: utf-8 -*-
"""chapter_4_sort_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xx5H9JBm_xcE6uZ9YsCCPYVf1A5ekm1Y

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Overview" data-toc-modified-id="Overview-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href="#Learning-Objectives" data-toc-modified-id="Learning-Objectives-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Learning Objectives</a></span></li><li><span><a href="#Bubble-Sort" data-toc-modified-id="Bubble-Sort-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Bubble Sort</a></span><ul class="toc-item"><li><span><a href="#The-brute-force-idea" data-toc-modified-id="The-brute-force-idea-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span>The brute-force idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-brute-force-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-brute-force-implementation-3.2"><span class="toc-item-num">3.2&nbsp;&nbsp;</span>Exercise ($\star\star$): the brute-force implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.3"><span class="toc-item-num">3.3&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#An-improved-idea" data-toc-modified-id="An-improved-idea-3.4"><span class="toc-item-num">3.4&nbsp;&nbsp;</span>An improved idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-improved-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-improved-implementation-3.5"><span class="toc-item-num">3.5&nbsp;&nbsp;</span>Exercise ($\star\star$): the improved implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.6"><span class="toc-item-num">3.6&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#The-optimal-idea" data-toc-modified-id="The-optimal-idea-3.7"><span class="toc-item-num">3.7&nbsp;&nbsp;</span>The optimal idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-optimal-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-optimal-implementation-3.8"><span class="toc-item-num">3.8&nbsp;&nbsp;</span>Exercise ($\star\star$): the optimal implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.9"><span class="toc-item-num">3.9&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li><li><span><a href="#Selection-Sort" data-toc-modified-id="Selection-Sort-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Selection Sort</a></span><ul class="toc-item"><li><span><a href="#The-brute-force-idea" data-toc-modified-id="The-brute-force-idea-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>The brute-force idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-brute-force-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-brute-force-implementation-4.2"><span class="toc-item-num">4.2&nbsp;&nbsp;</span>Exercise ($\star\star$): the brute-force implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-4.3"><span class="toc-item-num">4.3&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#An-improved-idea" data-toc-modified-id="An-improved-idea-4.4"><span class="toc-item-num">4.4&nbsp;&nbsp;</span>An improved idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-improved-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-improved-implementation-4.5"><span class="toc-item-num">4.5&nbsp;&nbsp;</span>Exercise ($\star\star$): the improved implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-4.6"><span class="toc-item-num">4.6&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#The-optimal-idea" data-toc-modified-id="The-optimal-idea-4.7"><span class="toc-item-num">4.7&nbsp;&nbsp;</span>The optimal idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-optimal-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-optimal-implementation-4.8"><span class="toc-item-num">4.8&nbsp;&nbsp;</span>Exercise ($\star\star$): the optimal implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-4.9"><span class="toc-item-num">4.9&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li><li><span><a href="#Insertion-Sort" data-toc-modified-id="Insertion-Sort-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Insertion Sort</a></span><ul class="toc-item"><li><span><a href="#The-brute-force-idea" data-toc-modified-id="The-brute-force-idea-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>The brute-force idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-brute-force-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-brute-force-implementation-5.2"><span class="toc-item-num">5.2&nbsp;&nbsp;</span>Exercise ($\star\star$): the brute-force implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-5.3"><span class="toc-item-num">5.3&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#The-optimal-idea" data-toc-modified-id="The-optimal-idea-5.4"><span class="toc-item-num">5.4&nbsp;&nbsp;</span>The optimal idea</a></span></li><li><span><a href="#Exercise-($\star\star$):-the-optimal-implementation" data-toc-modified-id="Exercise-($\star\star$):-the-optimal-implementation-5.5"><span class="toc-item-num">5.5&nbsp;&nbsp;</span>Exercise ($\star\star$): the optimal implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-5.6"><span class="toc-item-num">5.6&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li><li><span><a href="#Merge-Sort" data-toc-modified-id="Merge-Sort-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Merge Sort</a></span><ul class="toc-item"><li><span><a href="#The-idea" data-toc-modified-id="The-idea-6.1"><span class="toc-item-num">6.1&nbsp;&nbsp;</span>The idea</a></span></li><li><span><a href="#Exercise-($\star\star\star$):-the-implementation" data-toc-modified-id="Exercise-($\star\star\star$):-the-implementation-6.2"><span class="toc-item-num">6.2&nbsp;&nbsp;</span>Exercise ($\star\star\star$): the implementation</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-6.3"><span class="toc-item-num">6.3&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li><li><span><a href="#Quick-sort" data-toc-modified-id="Quick-sort-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Quick sort</a></span><ul class="toc-item"><li><span><a href="#The-idea" data-toc-modified-id="The-idea-7.1"><span class="toc-item-num">7.1&nbsp;&nbsp;</span>The idea</a></span></li><li><span><a href="#Exercise-($\star\star\star$):-the-implementation-(with-the-last-element-as-the-pivot)" data-toc-modified-id="Exercise-($\star\star\star$):-the-implementation-(with-the-last-element-as-the-pivot)-7.2"><span class="toc-item-num">7.2&nbsp;&nbsp;</span>Exercise ($\star\star\star$): the implementation (with the last element as the pivot)</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-7.3"><span class="toc-item-num">7.3&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#Exercise-($\star\star\star$):-the-implementation-(with-the-median-element-as-the-pivot)" data-toc-modified-id="Exercise-($\star\star\star$):-the-implementation-(with-the-median-element-as-the-pivot)-7.4"><span class="toc-item-num">7.4&nbsp;&nbsp;</span>Exercise ($\star\star\star$): the implementation (with the median element as the pivot)</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-7.5"><span class="toc-item-num">7.5&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li></ul></div>

<b>

<p>
<center>
<font size="5">
Lecture Note for Computer Science Foundations (DATS 6450)
</font>
</center>
</p>

<p>
<center>
<font size="4">
Chapter 4: Sort (Solution)
</font>
</center>
</p>

<p>
<center>
<font size="3">
Data Science, Columbian College of Arts & Sciences, George Washington University
</font>
</center>
</p>

<p>
<center>
<font size="3">
Author: Yuxiao Huang
</font>
</center>
</p>

</b>

# Overview
- We will discuss five sorting algorithms here:
    - Bubble Sort
    - Selection Sort
    - Insertion Sort
    - Merge Sort
    - Quick Sort
- Particularly, the discussion of each algorithm can be divided into two parts:
    - theory, where we will describe the idea of the algorithm
    - coding
        - where (most of the time) we will start with some examples and then work on some exercises
        - particularly, the examples and exercises are organized in such a way that, an exercise (most of the time) is a follow-up on some example prior to it
        - **you should analyze the (time and space) complexity of each example and exercise**
- We will use stars to represent the difficulty of the exercises:
    - $\star$ means very easy
    - $\star\star$ means easy
    - $\star\star\star$ means medium
    - $\star\star\star\star$ means difficult
    - $\star\star\star\star\star$ means very difficult

# Learning Objectives
Students should know:
- the idea of each kind of sorting algorithm
- the implementation of each kind of sorting algorithm

# Bubble Sort

## The brute-force idea
*Bubble sort* is one of the simplest sorting algorithms, and we briefly discussed the algorithm in Chapter 1. The idea is that, we pass the input array (from left to right) $n$ times (where $n$ is the length of the array). In each pass, we swap two adjacent numbers if the first number is larger than the second. By doing this:
- in the first round, the largest number will be moved to the last place of the array
- in the second round, the second largest number will be moved to the second last place of the array
- in the $i$th round, the $i$th largest number will be moved to the $i$th last place of the array
- in the $n$th round (the last round), the smallest number will be moved to the first place of the array

## Exercise ($\star\star$): the brute-force implementation
- Problem:
    - based on the brute-force idea above, implement the bubble sort algorithm
- Logic: see the brute-force idea above
"""

def bubble_sort_brute_force(arr):
    """
    Bubble sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """
    
    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)
    
    for i in range(n):
        for j in range(n - 1):
            # Get the difference between two adjacent numbers
            diff = arr_temp[j] - arr_temp[j + 1]
            if diff > 0:
                # Swap the two numbers
                arr_temp[j], arr_temp[j + 1] = arr_temp[j + 1], arr_temp[j]               

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(bubble_sort_brute_force(arr_1))
print(bubble_sort_brute_force(arr_2))
print(bubble_sort_brute_force(arr_3))
print(bubble_sort_brute_force(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the brute-force implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

##  An improved idea
While the brute-force implementation works, it is not efficient since some of the computations are not necessary. For instance, as discussed earlier, in the first round the largest number will be moved to the last place of the array. Thus:
- in the second round, we do not have to consider the last element in the array since it is the top-1 largest, established in the first round
- in the third round, we do not have to consider the last two elements in the array since they are the top-2 largest, established in the first two rounds
- in the $i$th round, we do not have to consider the last $i - 1$ elements in the array since they are the top-$(i - 1)$ largest, established in the first $i - 1$ rounds
- in the $n$th round (the last round), we do not have to consider the last $n - 1$ elements in the array since they are the top-$(n - 1)$ largest, established in the first $n - 1$ rounds

The observations above say that, in the $i$th round of bubble sort, we can ignore the last $i - 1$ elements in the array and focus on the first $n - i + 1$ elements. This will avoid some of the unnecessary computations.

## Exercise ($\star\star$): the improved implementation
- Problem:
    - based on the improved idea above, implement the bubble sort algorithm
- Logic: see the improved idea above
"""

def bubble_sort_improved(arr):
    """
    Bubble sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """
    
    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)
    
    for i in range(n):
        for j in range(n - i - 1):
            # Get the difference between two adjacent numbers
            diff = arr_temp[j] - arr_temp[j + 1]
            if diff > 0:
                # Swap the two numbers
                arr_temp[j], arr_temp[j + 1] = arr_temp[j + 1], arr_temp[j]               

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(bubble_sort_improved(arr_1))
print(bubble_sort_improved(arr_2))
print(bubble_sort_improved(arr_3))
print(bubble_sort_improved(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the improved implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

The difference (in run time) between the brute-force and improved implementation of bubble sort is shown in figs. 1 to 3 (where the input array with respect to figs. 1 and 2 are sorted and that with respect to fig. 3 is not sorted). This experimentally demonstrated what we theoretically explained. That is, while the two kinds of solutions have the same time complexity, $O(n^2$), the improved one is more efficient than the brute-force one.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import time
# %matplotlib inline
import matplotlib.pyplot as plt

def plot(n, funs, sorted_, replace_str):
    """
    Plot the run time of the functions (with respect to the input size).
    
    Parameters
    ----------
    n : an integer
    funs : a list of functions
    sorted_ : an integer which says:
              -1 : sorted in descending order
              0 : not sorted
              1 : sorted in ascending order
    replace_str : a string to be replaced by ''
    """
    
    np.random.seed(0)
    
    x = list(range(1, n + 1))
    ys = [[] for _ in range(len(funs))]

    for i in x:
        arr = list(range(1, i + 1))
        if sorted_ == -1:
            arr = sorted(arr, reverse=True)
        elif sorted_ == 0:
            np.random.shuffle(arr)
        for j in range(len(funs)):
            start = time.time()
            if 'quick_sort' in funs[j].__name__:
                funs[j](arr, 0, len(arr) - 1)
            else:
                funs[j](arr)
            end = time.time()
            ys[j].append(end - start)
    
    for j in range(len(funs)):
        plt.plot(x, ys[j], label=funs[j].__name__.replace(replace_str, ''))
    plt.xlabel('$n$', fontsize=20)
    plt.ylabel('Run time', fontsize=20)
    plt.xticks([min(x), max(x)], fontsize=20)
    plt.yticks([min([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))]), max([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))])], fontsize=20)
    plt.legend(fontsize=20)
    plt.tight_layout()
    plt.show()

plot(10 ** 2, [bubble_sort_brute_force, bubble_sort_improved], -1, 'bubble_sort_')
print("Figure 1. The run time of bubble_sort_brute_force and bubble_sort_improved when the input array is sorted in descending order.")

plot(10 ** 2, [bubble_sort_brute_force, bubble_sort_improved], 1, 'bubble_sort_')
print("Figure 2. The run time of bubble_sort_brute_force and bubble_sort_improved when the input array is sorted in ascending order.")

plot(10 ** 2, [bubble_sort_brute_force, bubble_sort_improved], 0, 'bubble_sort_')
print("Figure 3. The run time of bubble_sort_brute_force and bubble_sort_improved when the input array is not sorted.")

"""## The optimal idea
While the improved implementation is more efficient than the brute-force one, it is still not the optimal solution. To understand why this is the case, let us assume the input array has already been sorted (in ascending order). Ideally, after passing the array once and finding the array has been sorted, the sorting algorithm should do nothing but simply return the sorted array. However, the improved implementation of bubble sort will always pass the array $n$ times even when the array has already been sorted, resulting in unnecessary computation.

Based on this observation, the improved implementation can be further improved by checking whether the array has already been sorted after passing the array in each round. We terminate the iteration as soon as we find the array has been sorted. The question is, how shall we do this in bubble sort?

To answer this question, let us briefly review how bubble sort works. In each round, it compares each adjacent two numbers in the array and swaps the two if the first number is larger than the second. Now think about this question: what does it mean when no number was swapped in a round?

Basically, it says that each number in the array was no larger than the number behind it (on its right). It is because if it had not been the case, at least one pair of numbers would have been swapped. Based on this idea, we can use a flag variable (a.k.a., sentinel variable or signal variable) to indicate whether there are numbers that were swapped. If there were, we will continue the loop. Otherwise, (as discussed earlier) we will terminate the loop.

## Exercise ($\star\star$): the optimal implementation
- Problem:
    - based on the optimal idea above, implement the bubble sort algorithm
- Logic: see the optimal idea above
"""

def bubble_sort_optimal(arr):
    """
    Bubble sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """

    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)
    
    for i in range(n):
        # If there are numbers that were swapped, False by default
        swapped = False
        
        for j in range(n - i - 1):
            # Get the difference between two adjacent numbers
            diff = arr_temp[j] - arr_temp[j + 1]
            if diff > 0:
                # Swap the two numbers
                arr_temp[j], arr_temp[j + 1] = arr_temp[j + 1], arr_temp[j]    
                
                # There are numbers that were swapped
                swapped = True
        
        # If there were no numbers that were swapped
        if swapped is False:
            break

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(bubble_sort_optimal(arr_1))
print(bubble_sort_optimal(arr_2))
print(bubble_sort_optimal(arr_3))
print(bubble_sort_optimal(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the optimal implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

The difference (in run time) between the improved and optimal implementation of bubble sort is shown in figs. 4 to 6. It is worth noting that while there is no significant difference between the two kinds of implementation when the input array is sorted in descending order (fig. 4) or not sorted (fig. 5), the optimal one is significantly much faster when the array is sorted in ascending order (fig. 6). The reason for this improvement was discussed earlier.
"""

plot(10 ** 2, [bubble_sort_improved, bubble_sort_optimal], -1, 'bubble_sort_')
print("Figure 4. The run time of bubble_sort_improved and bubble_sort_optimal when the input array is sorted in descending order.")

plot(10 ** 2, [bubble_sort_improved, bubble_sort_optimal], 0, 'bubble_sort_')
print("Figure 5. The run time of bubble_sort_improved and bubble_sort_optimal when the input array is not sorted.")

plot(10 ** 2, [bubble_sort_improved, bubble_sort_optimal], 1, 'bubble_sort_')
print("Figure 6. The run time of bubble_sort_improved and bubble_sort_optimal when the input array is sorted in ascending order.")

"""# Selection Sort

## The brute-force idea
It turns out that the second kind of sorting algorithm, *selection sort*, is very similar to bubble sort. The difference is, unlike bubble sort that finds the largest element (in each round), selection sort finds the smallest. This means that, in its simplest form, we can reverse bubble sort to obtain selection sort. Specifically, in each round:
- instead of passing the input array from left to right (as in bubble sort), we do so from right to left
- instead of pushing the current largest element to the right (as in bubble sort), we push the smallest to the left

## Exercise ($\star\star$): the brute-force implementation
- Problem:
    - based on the brute-force idea above, implement the selection sort algorithm
- Logic: see the brute-force idea above
"""

def selection_sort_brute_force(arr):
    """
    Selection sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """

    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)
    
    for i in range(n):
        for j in range(n - 1, 0, -1):
            # Get the difference between two adjacent numbers
            diff = arr_temp[j] - arr_temp[j - 1]
            if diff < 0:
                # Swap
                arr_temp[j], arr_temp[j - 1] = arr_temp[j - 1], arr_temp[j]               

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(selection_sort_brute_force(arr_1))
print(selection_sort_brute_force(arr_2))
print(selection_sort_brute_force(arr_3))
print(selection_sort_brute_force(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the brute-force implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

The difference (in run time) between the brute-force implementation of bubble sort and selection sort is shown in figs. 7 to 9. It is not surprising that there is no significant difference since they are essentially the same.
"""

plot(10 ** 2, [bubble_sort_brute_force, selection_sort_brute_force], -1, '')
print("Figure 7. The run time of bubble_sort_brute_force and selection_sort_brute_force when the input array is sorted in descending order.")

plot(10 ** 2, [bubble_sort_brute_force, selection_sort_brute_force], 1, '')
print("Figure 8. The run time of bubble_sort_brute_force and selection_sort_brute_force when the input array is sorted in ascending order.")

plot(10 ** 2, [bubble_sort_brute_force, selection_sort_brute_force], 0, '')
print("Figure 9. The run time of bubble_sort_brute_force and selection_sort_brute_force when the input array is not sorted.")

"""## An improved idea
The same as the brute-force implementation for bubble sort, passing each element in each round is not necessary. The reason is that:
- in the second round, we do not have to consider the first element in the array since it is the top-1 smallest, established in the first round
- in the third round, we do not have to consider the first two elements in the array since they are the top-2 smallest, established in the first two rounds
- in the $i$th round, we do not have to consider the first $i - 1$ elements in the array since they are the top-$(i - 1)$ smallest, established in the first $i - 1$ rounds
- in the $n$th round (the last round), we do not have to consider the first $n - 1$ elements in the array since they are the top-$(n - 1)$ smallest, established in the first $n - 1$ rounds

The observations above say that, in the $i$th round of selection sort, we can ignore the first $i - 1$ elements in the array and focus on the last $n - i + 1$ elements. This will avoid some of the unnecessary computations.

## Exercise ($\star\star$): the improved implementation
- Problem:
    - based on the improved idea above, implement the selection sort algorithm
- Logic: see the improved idea above
"""

def selection_sort_improved(arr):
    """
    Selection sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """

    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)
    
    for i in range(n):
        for j in range(n - 1, i, -1):
            # Get the difference between two adjacent numbers
            diff = arr_temp[j] - arr_temp[j - 1]
            if diff < 0:
                # Swap the two numbers
                arr_temp[j], arr_temp[j - 1] = arr_temp[j - 1], arr_temp[j]               

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(selection_sort_improved(arr_1))
print(selection_sort_improved(arr_2))
print(selection_sort_improved(arr_3))
print(selection_sort_improved(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the improved implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

The difference (in run time) between the brute-force and improved implementation of selection sort can be seen in figs. 10 to 12. The improved one is always faster no matter whether the input array is sorted or not.
"""

plot(10 ** 2, [selection_sort_brute_force, selection_sort_improved], -1, 'selection_sort_')
print("Figure 10. The run time of selection_sort_brute_force, selection_sort_improved when the input array is sorted in descending order.")

plot(10 ** 2, [selection_sort_brute_force, selection_sort_improved], 1, 'selection_sort_')
print("Figure 11. The run time of selection_sort_brute_force, selection_sort_improved when the input array is sorted in ascending order.")

plot(10 ** 2, [selection_sort_brute_force, selection_sort_improved], 0, 'selection_sort_')
print("Figure 12. The run time of selection_sort_brute_force, selection_sort_improved when the input array is not sorted.")

"""## The optimal idea
While the improved implementation is faster than the brute-force one, it still has a problem. In order to know what the problem is, we need to briefly review how selection sort works. The main idea is that, for each iteration we first find the smallest element in the remaining subarray and then push it to the beginning of the subarray. In the improved implementation, this is done by comparing each two adjacent elements and swap the two if the one on the right is smaller than the one on the left. Now think about the following two questions:
- what is the worst case that results in the maximum number of swap?
- what is this maximum number of swap?

It turns out that the worst case is the one where the array is sorted in descending order. In this case we have to make $n - 1$ swap in each round of selection sort (where $n$ is the number of elements in the subarray) since for each two adjacent elements, the one on the right is smaller than the one on the left. As mentioned earlier, since the whole point for each pass is to find the smallest element and push it to the beginning of the subarray, we can simply keep track of the (index of the) smallest element when passing the subarray. Then after passing the subarray, we can simply swap the smallest element and the first element (of the subarray). By doing this, we only need one swap in the worst case (where we need $n - 1$ swaps in the improved implementation).

## Exercise ($\star\star$): the optimal implementation
- Problem:
    - based on the optimal idea above, implement the selection sort algorithm
- Logic: see the optimal idea above
"""

def selection_sort_optimal(arr):
    """
    Selection sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """
 
    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)
    
    for i in range(n):
        # Initialize the min number and its index
        min_num, min_idx = arr_temp[n - 1], n - 1
        
        for j in range(n - 2, i - 1, -1):
            # Get the difference between the current number and min_num
            diff = arr_temp[j] - min_num
            if diff < 0:
                # Update min_num and min_idx
                min_num, min_idx = arr_temp[j], j
        
        # Swap the smallest element and the first element
        arr_temp[i], arr_temp[min_idx] = arr_temp[min_idx], arr_temp[i]               

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(selection_sort_optimal(arr_1))
print(selection_sort_optimal(arr_2))
print(selection_sort_optimal(arr_3))
print(selection_sort_optimal(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the optimal implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

The difference (in run time) between the improved and optimal implementation can be seen in figs. 13 to 15. While the two are almost the same when the array is sorted in ascending order (fig. 13), the optimal one is much faster when the array is sorted in descending order (fig. 14) or not sorted (fig. 15). Can you explain why?
"""

plot(10 ** 2, [selection_sort_improved, selection_sort_optimal], 1, 'selection_sort_')
print("Figure 13. The run time of selection_sort_improved and selection_sort_optimal when the input array is sorted in ascending order.")

plot(10 ** 2, [selection_sort_improved, selection_sort_optimal], -1, 'selection_sort_')
print("Figure 14. The run time of selection_sort_improved and selection_sort_optimal when the input array is sorted in descending order.")

plot(10 ** 2, [selection_sort_improved, selection_sort_optimal], 0, 'selection_sort_')
print("Figure 15. The run time of selection_sort_improved and selection_sort_optimal when the input array is not sorted.")

"""# Insertion Sort

## The brute-force idea
Let us move on to the third kind of sorting algorithm, *insertion sort*. Interestingly, while you may have not heard of the name of this sorting algorithm, I am pretty sure you have been using it a lot, particularly when you play cards. Let us think about how we sort cards in our hands (e.g., from left to right):
- we first look at the second card and compare it with the first one. If the second is larger than the first, it stays. Otherwise, we insert it before the first. After this, the first two cards have been sorted.
- we then look at the third card and compare it with the first two cards. If the third is larger than the first two, it stays. Otherwise, we insert it before a card such that the card is the first one larger than it. After this, the first three cards have been sorted.
- we look at the $i$th card and compare it with the first $i - 1$ cards. If the $i$th card is larger than the first $i - 1$ cards, it stays. Otherwise, we insert it before a card such that the card is the first one larger than it. After this, the first $i$ cards have been sorted.
- we repeat the above steps for all the remaining cards. After this, all the cards in our hands have been sorted. 

It turns out that, the above steps we use for sorting cards are the same steps in insertion sort. A tiny difference is, when sorting cards we can simply insert a card before others and all the cards behind the inserted one will be moved one step to the right. However, when sorting numbers in an input array, we have to manually move numbers after the insertion.

## Exercise ($\star\star$): the brute-force implementation
- Problem:
    - based on the brute-force idea above, implement the insertion sort algorithm
- Logic: see the brute-force idea above
"""

def insertion_sort_brute_force(arr):
    """
    Insertion sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """

    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)
    
    for i in range(1, n):
        # A copy of arr[i]
        temp = arr_temp[i]
        
        # Find the first element, arr_temp[j], which is no smaller than arr_temp[i]
        j = 0
        while j < i and arr_temp[j] < temp:
            j += 1

        # Move each element between arr_temp[j] and arr_temp[i - 1] one step to the right
        for k in range(i - 1, j - 1, -1):
            arr_temp[k + 1] = arr_temp[k]

        # Insert arr_temp[i] at j
        arr_temp[j] = temp

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(insertion_sort_brute_force(arr_1))
print(insertion_sort_brute_force(arr_2))
print(insertion_sort_brute_force(arr_3))
print(insertion_sort_brute_force(arr_4))

"""## Discussion

**Q**: What is the time and space complexity of the brute-force implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

## The optimal idea
While the above brute-force implementation works, it is not efficient since for each $i$ the solution requires at least $i$ steps. This is because we first need to find the first element, arr_temp[j], which is no smaller than arr_temp[i]. This takes $j$ steps. We then need to move each element between arr_temp[j] and arr_temp[i - 1] one step to the right (to insert arr_temp[i] at $j$). This takes $i - j$ steps. As a result, the two steps above take $j + i - j = i$ steps. It is worth nothing that this claim is always true regardless whether the input array is sorted or not. The reason is that, for each $i$ we loop over the elements from left to right. Thus after finding arr_temp[j], we need to move each element between arr_temp[j] and arr_temp[i - 1]. 

However, the sorting algorithm could be more efficient if we loop over elements in the opposite way, that is, from right to left. The idea is that, for round $i$ we pass the smaller indices, $j$, from right to left. If arr_temp[i] < arr_temp[j], we move arr_temp[j] one step to the right. We repeat this until arr_temp[i] >= arr_temp[j]. When this is the case, we insert arr_temp[i] at $j + 1$ and then terminate round $i$.

## Exercise ($\star\star$): the optimal implementation
- Problem:
    - based on the optimal idea above, implement the insertion sort algorithm
- Logic: see the optimal idea above
"""

def insertion_sort_optimal(arr):
    """
    Insertion sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """
    
    # Implement me    
    # A copy of the list
    arr_temp = list(arr) 
    n = len(arr_temp)
    
    for i in range(1, n):
        # A copy of arr[i]
        temp = arr_temp[i]

        # Find the last element, arr_temp[j], which is no larger than arr_temp[i]
        j = i - 1
        while j >= 0 and arr[j] > temp:
            # Move arr_temp[j] one step to the right
            arr_temp[j + 1] = arr_temp[j]      
            j -= 1

        # Insert arr[i] at j + 1
        arr_temp[j + 1] = temp

    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(insertion_sort_optimal(arr_1))
print(insertion_sort_optimal(arr_2))
print(insertion_sort_optimal(arr_3))
print(insertion_sort_optimal(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the optimal implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

The difference (in run time) between the brute-force and optimal implementation of insertion sort can be seen in figs. 16 to 18. It is worth noting that the optimal solution is actually slower than the brute-force one (fig. 16). This is because here the input array is sorted in descending order. This is the worst case for the optimal solution, which takes around $2 \times i$ steps for each round $i$ (can you explain why?). As discussed earlier, the brute-force solution always takes around $i$ steps for each round $i$, regardless whether the input array is sorted or not. This is the reason why it is faster than the optimal one.

However, the brute-force implementation is much slower than the optimal one when the input array is sorted in ascending order (fig. 17). This is the best case for the optimal solution, which only takes around 2 steps for each round $i$ (can you explain why?). The brute-force solution, on the other hand, again takes around $i$ steps in this case. This is the reason why it is much slower than the optimal one.

It seems that we have a tie here. The brute-force implementation is faster than the optimal one when the input array is sorted in descending order (the worst case for the optimal one), but slower when the array is sorted in ascending order (the best case). Then the question is, is the optimal solution actually better than the brute-force one?

The answer is, generally the optimal solution is indeed better. This can be seen in fig. 18, where the input array is not sorted and the optimal solution is faster than the brute-force one. This breaks the tie discussed earlier. Actually, we should be more interested in comparing the two kinds of solutions when the array is not sorted, since it measures what the array usually looks like (or, in other words, the average case of the input). As a result, we can say averagely the optimal solution is faster than the brute-force one. 
"""

plot(10 ** 2, [insertion_sort_brute_force, insertion_sort_optimal], -1, 'insertion_sort_')
print("Figure 16. The run time of insertion_sort_brute_force and insertion_sort_optimal when the input array is sorted in descending order.")

plot(10 ** 2, [insertion_sort_brute_force, insertion_sort_optimal], 1, 'insertion_sort_')
print("Figure 17. The run time of insertion_sort_brute_force and insertion_sort_optimal when the input array is sorted in ascending order.")

plot(10 ** 2, [insertion_sort_brute_force, insertion_sort_optimal], 0, 'insertion_sort_')
print("Figure 18. The run time of insertion_sort_brute_force and insertion_sort_optimal when the input array is not sorted.")

"""# Merge Sort
Before moving on to the next sorting algorithm, let us briefly review what we have learned so far. We discussed three kinds of algorithms, including bubble sort, selection sort and insertion sort. For each algorithm, we started from the brute-force implementation and ended with the optimal one. While there are many differences between the three algorithms, there is something in common, that is, the time and space complexity. Specifically, for all of the three algorithms, the time and space complexity are $O(n^2)$ and $O(n)$.

## The idea

The sorting algorithm we will discuss in this section is *Merger Sort*. It is a classic divide-and-conquer approach, which as we mentioned in Chapter 3, solves a big problem by first dividing the problem into some small subproblems and then solving the small subproblems. Specifically, a divide-and-conquer algorithm can be summarized as

def fun(n):

    do some work to divide the problem into subproblems
    
    for _ in range(a):
        fun(n / b)
        
    do some work to merge the result of the subproblems
    
where:
- $n$ is the size of the problem
- $a$ is the number of subproblems that need to be solved
- $n / b$ the size of each subproblem

The bulk of the algorithm is dividing the original problem (of size $n$) into ($a$) subproblems (of size $n / b$) and solving the subproblems. We also need to do some preparation before the division, and some summarization after solving all the subproblems.

Based on this paradigm, in merge sort we first divide the input array into two subarrays. We then recursively use the algorithm to sort the two subarrys. Last, we merge the two sorted subarries into one.

## Exercise ($\star\star\star$): the implementation
- Problem:
    - based on the idea above, implement the merge sort algorithm
- Logic: see the idea above
"""

def merge_sort(arr):
    """
    Merge sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    The list sorted in ascending order
    """
    
    # Implement me  
    # A copy of the list
    arr_temp = list(arr)
    n = len(arr_temp)    
    
    if n > 1: 
        # Divide the list into two smaller ones
        # The middle of the list
        mid = n // 2
        # The left sublist
        arr_temp_left = arr_temp[:mid] 
        # The right sublist
        arr_temp_right = arr_temp[mid:]
  
        # Recursively call merge_sort to sort the two smaller lists
        arr_temp_left = merge_sort(arr_temp_left)
        arr_temp_right = merge_sort(arr_temp_right)
          
        # Merge the two sorted smaller lists
        i = j = k = 0
        n_left, n_right = len(arr_temp_left), len(arr_temp_right)
          
        while i < n_left and j < n_right: 
            if arr_temp_left[i] < arr_temp_right[j]: 
                arr_temp[k] = arr_temp_left[i] 
                i += 1
            else: 
                arr_temp[k] = arr_temp_right[j] 
                j += 1
            k += 1
          
        # If there are elements in arr_temp_left that have not been visited 
        while i < n_left: 
            arr_temp[k] = arr_temp_left[i] 
            i += 1
            k += 1
 
        # If there are elements in arr_temp_right that have not been visited 
        while j < n_right: 
            arr_temp[k] = arr_temp_right[j] 
            j += 1
            k += 1
            
    return arr_temp

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(merge_sort(arr_1))
print(merge_sort(arr_2))
print(merge_sort(arr_3))
print(merge_sort(arr_4))

"""## Discussion
**Q**: What is the time and space complexity of the implementation above?

**A**: 
- $O(n \log(n))$ time
- $O(n)$ space

As discussed in Chapters 1 and 2, the space complexity can be obtained by analyzing the extra space required in the longest path of the recursion tree. While we can also use the recursion tree to obtain the time complexity, there is a more convenient way to do so (can you figure out what that is?).

The difference (in run time) between the first four kinds of algorithms can be seen in figs. 19 to 21. An interesting finding is that, while merge sort is the fastest when the input array is sorted in descending order, it is slower than insertion sort when the array is sorted in ascending order or not sorted. This is counterintuitive because in theory merge sort should be faster than all the other three methods, since its time complexity is log linear whereas that of the others is quadratic. The reason why merge sort is slower is that, we have to make a copy of the input array (line 15 in the implementation above) and the sorted subarrays (lines 28 and 29). These overhead lower the speed of the algorithm.
"""

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort], -1, '')
print("Figure 19. The run time of bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, and merge_sort when the input array is sorted in descending order.")

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort], 1, '')
print("Figure 20. The run time of bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, and merge_sort when the input array is sorted in ascending order.")

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort], 0, '')
print("Figure 21. The run time of bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, and merge_sort when the input array is not sorted.")

"""# Quick sort

## The idea
The last sorting algorithm we will discuss in this chapter is called *Quick Sort*. The idea of the algorithm is quite simple:
- We first choose a number from the array as the pivot
- We then sort the array in such a way that:
    - every element on the left-hand side of the pivot is no larger than the pivot
    - every element on the right-hand side of the pivot is larger than the pivot
- We recursively apply the above steps to the left and right subarray

## Exercise ($\star\star\star$): the implementation (with the last element as the pivot)
- Problem:
    - based on the idea above, implement the quick sort algorithm
    - here we use the last element of the input array as the pivot.
- Logic: see the idea above
"""

def quick_sort_last(arr, left, right):
    """
    Quick sort (using the last element of the input array as the pivot)
    
    Parameters
    ----------
    arr : a list of number
    left : the starting index of the list
    right : the ending index of the list

    Returns
    ----------
    The list sorted in ascending order
    """

    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    
    if left < right: 
        # Partition
        # All the number no larger than the pivot will be on the left-hand side of the pivot
        # All the number larger than the pivot will be on the right-hand side of the pivot
        pivot = partition_last(arr_temp, left, right) 
  
        # Sort the left and right sublist
        arr_temp = quick_sort_last(arr_temp, left, pivot - 1) 
        arr_temp = quick_sort_last(arr_temp, pivot + 1, right) 
        
    return arr_temp

def partition_last(arr, left, right):
    """
    Partition (using the last element of the input array as the pivot)
    All the number no larger than the pivot will be on the left-hand side of the pivot
    All the number larger than the pivot will be on the right-hand side of the pivot 
    
    Parameters
    ----------
    arr : a list of number
    left : the starting index of the list
    right : the ending index of the list

    Returns
    ----------
    The index of the pivot
    """
    
    # Implement me
    # The pivot (using the last element of the input array as the pivot)
    pivot = arr[right]    
    
    # The index of elements no larger than pivot
    i = left - 1 
  
    for j in range(left, right): 
        if arr[j] <= pivot: 
            i = i + 1 
            # Swap the two numbers
            arr[i], arr[j] = arr[j], arr[i] 
    
    # Rearrange the pivot
    arr[i + 1], arr[right] = arr[right], arr[i + 1] 
    
    # Return the index of the pivot
    return i + 1

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(quick_sort_last(arr_1, 0, len(arr_1) - 1))
print(quick_sort_last(arr_2, 0, len(arr_2) - 1))
print(quick_sort_last(arr_3, 0, len(arr_3) - 1))
print(quick_sort_last(arr_4, 0, len(arr_4) - 1))

"""## Discussion
**Q**: What is the time and space complexity of the implementation above?

**A**: 
- $O(n^2)$ time
- $O(n)$ space

It is worth noting that, unlike merge sort whose time complexity is log linear, the time complexity of quick sort is quadratic. This is because in the worst case where the input array is sorted in ascending order, the pivot will always be the last element of the array. In turn, we have to pass the array $n$ times and the complexity of each time is $O(n)$ (can you explain why?). However, in the average case where the array is not sorted, the time complexity of quick sort is $O(n \log(n))$. This is because in this case the pivot could be quite close to the middle of the array. Using the master theorem, we can show that the time complexity is log linear (try to do this yourself).

The difference (in run time) between quick sort (with the last element being the pivot) and the other four kinds of soring algorithms can be seen in fig. 22 to 24. As shown in the figures, while quick sort is faster than merge sort when the input array is not sorted (fig. 22), it is slower when the array is sorted (figs. 23 and 24). Particularly, when the array is sorted in ascending order, quick sort is the slowest among the five (fig. 23). As discussed earlier, this is the worst case for quick sort and the time complexity is $O(n^2)$. This problem can somehow be alleviated by using the median element of the input array as the pivot.
"""

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort, quick_sort_last], 0, '')
print("Figure 22. The run time of the five sorting algorithms when the input array is not sorted.")

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort, quick_sort_last], 1, '')
print("Figure 23. The run time of the five sorting algorithms when the input array is sorted in ascending order.")

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort, quick_sort_last], -1, '')
print("Figure 24. The run time of the five sorting algorithms when the input array is sorted in descending order.")

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort, quick_sort_last], 0, '')
print("Figure 24. The run time of the four sorting algorithms when the input array is not sorted.")

"""## Exercise ($\star\star\star$): the implementation (with the median element as the pivot)
- Problem:
    - based on the idea above, implement the quick sort algorithm
    - here we use the median (middle) element of the input array as the pivot.
- Logic: see the idea above
"""

def quick_sort_median(arr, left, right):
    """
    Quick sort (using the median element of the input array as the pivot)
    
    Parameters
    ----------
    arr : a list of number
    left : the starting index of the list
    right : the ending index of the list

    Returns
    ----------
    The list sorted in ascending order
    """
    
    # Implement me
    # A copy of the list
    arr_temp = list(arr)
    
    if left < right: 
        # Partition
        # All the number no larger than the pivot will be on the left-hand side of the pivot
        # All the number larger than the pivot will be on the right-hand side of the pivot
        pivot = partition_median(arr_temp, left, right) 
  
        # Sort the left and right sublist
        arr_temp = quick_sort_median(arr_temp, left, pivot - 1) 
        arr_temp = quick_sort_median(arr_temp, pivot + 1, right) 
        
    return arr_temp

def partition_median(arr, left, right):
    """
    Partition (using the last element of the input array as the pivot)
    All the number no larger than the pivot will be on the left-hand side of the pivot
    All the number larger than the pivot will be on the right-hand side of the pivot 
    
    Parameters
    ----------
    arr : a list of number
    left : the starting index of the list
    right : the ending index of the list

    Returns
    ----------
    The index of the pivot
    """
    
    # Implement me
    mid = left + (right - left) // 2
    arr[mid], arr[right] = arr[right], arr[mid]
    
    # The pivot (using the median element of the input array as the pivot)
    pivot = arr[right]    
    
    # The index of elements no larger than pivot
    i = left - 1 
  
    for j in range(left, right): 
        if arr[j] <= pivot: 
            i = i + 1 
            # Swap the two numbers
            arr[i], arr[j] = arr[j], arr[i] 

    # Rearrange the pivot
    arr[i + 1], arr[right] = arr[right], arr[i + 1] 
    
    # Return the index of the pivot
    return i + 1

# Test
arr_1 = []
arr_2 = [3]
arr_3 = [3, 2]
arr_4 = [3, 2, 1, 4]

print(quick_sort_median(arr_1, 0, len(arr_1) - 1))
print(quick_sort_median(arr_2, 0, len(arr_2) - 1))
print(quick_sort_median(arr_3, 0, len(arr_3) - 1))
print(quick_sort_median(arr_4, 0, len(arr_4) - 1))

"""## Discussion
As shown in fig. 25, when the input array is sorted in ascending order (the worst case discusser earlier) quick sort with the median element as the pivot is faster than the one with the last element as the pivot. However, quick sort is still not the fastest among the five sorting algorithms (fig. 26).
"""

plot(10 ** 2, [quick_sort_last, quick_sort_median], 1, '')
print("Figure 25. The run time of quick_sort_last and quick_sort_median when the input array is sorted in ascending order.")

plot(10 ** 2, [bubble_sort_optimal, selection_sort_optimal, insertion_sort_optimal, merge_sort, quick_sort_median], 1, '')
print("Figure 26. The run time of the five sorting algorithms when the input array is sorted in ascending order.")

