# -*- coding: utf-8 -*-
"""chapter_1_time_space_complexity_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jRwdWc23UtZ_1m-NCKuTFrstANtK4L9T

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Overview" data-toc-modified-id="Overview-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href="#Learning-Objectives" data-toc-modified-id="Learning-Objectives-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Learning Objectives</a></span></li><li><span><a href="#Time-Complexity" data-toc-modified-id="Time-Complexity-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Time Complexity</a></span></li><li><span><a href="#Common-Families-of-Complexity" data-toc-modified-id="Common-Families-of-Complexity-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Common Families of Complexity</a></span></li><li><span><a href="#Asymptotic-Order-of-Growth" data-toc-modified-id="Asymptotic-Order-of-Growth-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Asymptotic Order of Growth</a></span><ul class="toc-item"><li><span><a href="#The-big-O-notation" data-toc-modified-id="The-big-O-notation-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>The big O notation</a></span></li><li><span><a href="#Does-the-big-$O$-really-make-sense?" data-toc-modified-id="Does-the-big-$O$-really-make-sense?-5.2"><span class="toc-item-num">5.2&nbsp;&nbsp;</span>Does the big $O$ really make sense?</a></span></li><li><span><a href="#Exercises" data-toc-modified-id="Exercises-5.3"><span class="toc-item-num">5.3&nbsp;&nbsp;</span>Exercises</a></span><ul class="toc-item"><li><span><a href="#$\star$" data-toc-modified-id="$\star$-5.3.1"><span class="toc-item-num">5.3.1&nbsp;&nbsp;</span>$\star$</a></span></li><li><span><a href="#$\star$" data-toc-modified-id="$\star$-5.3.2"><span class="toc-item-num">5.3.2&nbsp;&nbsp;</span>$\star$</a></span></li><li><span><a href="#$\star\star$" data-toc-modified-id="$\star\star$-5.3.3"><span class="toc-item-num">5.3.3&nbsp;&nbsp;</span>$\star\star$</a></span></li><li><span><a href="#$\star\star\star$" data-toc-modified-id="$\star\star\star$-5.3.4"><span class="toc-item-num">5.3.4&nbsp;&nbsp;</span>$\star\star\star$</a></span></li></ul></li></ul></li><li><span><a href="#Best-and-Worst-Case-Analysis" data-toc-modified-id="Best-and-Worst-Case-Analysis-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Best and Worst Case Analysis</a></span></li><li><span><a href="#Space-Complexity" data-toc-modified-id="Space-Complexity-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Space Complexity</a></span><ul class="toc-item"><li><span><a href="#Exercises" data-toc-modified-id="Exercises-7.1"><span class="toc-item-num">7.1&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href="#A-Good-Way-to-Design-Algorithms" data-toc-modified-id="A-Good-Way-to-Design-Algorithms-8"><span class="toc-item-num">8&nbsp;&nbsp;</span>A Good Way to Design Algorithms</a></span><ul class="toc-item"><li><span><a href="#The-brute-force-solution" data-toc-modified-id="The-brute-force-solution-8.1"><span class="toc-item-num">8.1&nbsp;&nbsp;</span>The brute-force solution</a></span></li><li><span><a href="#Exercise-(an-improved-solution-$\star\star$)" data-toc-modified-id="Exercise-(an-improved-solution-$\star\star$)-8.2"><span class="toc-item-num">8.2&nbsp;&nbsp;</span>Exercise (an improved solution $\star\star$)</a></span></li><li><span><a href="#Exercise-(the-optimal-solution-$\star\star\star$)" data-toc-modified-id="Exercise-(the-optimal-solution-$\star\star\star$)-8.3"><span class="toc-item-num">8.3&nbsp;&nbsp;</span>Exercise (the optimal solution $\star\star\star$)</a></span></li></ul></li></ul></div>

<b>

<p>
<center>
<font size="5">
Lecture Note for Computer Science Foundations (DATS 6450)
</font>
</center>
</p>

<p>
<center>
<font size="4">
Chapter 1: Time and Space Complexity (Solution)
</font>
</center>
</p>

<p>
<center>
<font size="3">
Data Science, Columbian College of Arts & Sciences, George Washington University
</font>
</center>
</p>

<p>
<center>
<font size="3">
Author: Yuxiao Huang
</font>
</center>
</p>

</b>

# Overview
- We will start this course with introducing the time complexity (sections 3 to 6) and space complexity (section 7) of an algorithm
- We will particularly focus on the time complexity, which usually is the property of an algorithm we care about the most
- We will introduce a good way to design algorithms (section 8)
- We will use stars to represent the difficulty of the exercises:
    - $\star$ means very easy
    - $\star\star$ means easy
    - $\star\star\star$ means medium
    - $\star\star\star\star$ means difficult
    - $\star\star\star\star\star$ means very difficult

# Learning Objectives
After taking this class, students are expected to know:
- the meaning of time and space complexity
- some common families of complexity
- how to analyze the complexity of an algorithm

# Time Complexity
In simple words, the time complexity of an algorithm describes how fast the algorithm runs in theory. Instead of actually counting the number of steps in an algorithm or the run time of the algorithm, we first measure the order of growth of the algorithm (more on this later) and then divide algorithms into groups based on their order. By doing this, we can theoretically compare the speed of two algorithms by finding which family (of time complexity) they belong to, instead of actually running the algorithms. 

While there are other kinds of concerns (e.g., space complexity) when designing an algorithm, time complexity is usually the one we care about the most. This is why the main goal of this course is to design algorithms that have the lowest time complexity.

# Common Families of Complexity
While we have stressed the importance of time complexity several times already, we are pretty sure that you are not convinced. What is the big deal about it, you may wonder. Is an algorithm with low time complexity really better than one with high complexity?

To answer these questions, we need to talk about some common families of complexity. As mentioned previously, we can compare the speed of two algorithms by comparing the families they fall into. You will see very soon that some families are much better (faster) than the others.

Below are seven common families of complexity:
- constant: $O(1)$
- logarithmic: $O(\log n)$
- linear: $O(n)$
- log linear: $O(n \log n)$
- polynomial: $O(n^c)$
- exponential: $O(c^n)$
- factorial: $O(n!)$

Here, $n$ is the size of the input of an algorithm. The meaning of the big $O$ will be discussed in the next section. The list of families are sorted in ascending order of the complexity, with constant ($O(1)$) being the lowest (fastest) and factorial ($O(n!)$) the highest (slowest). Let us dive a bit deeper and see how some of these complexities change with the size of the input, $n$.

# <img src="../figure/figure_1.png" width="800" height="800">

Figure 1. The running times (rounded up) of different algorithms on inputs of increasing size. Picture courtesy of [*Algorithm Design*](https://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358).

As shown in fig. 1, the linear, log linear, and polynomial algorithms grow much slower (with $n$) than the exponential and factorial algorithms. Take $n = 100$ for example. For this moderately large input, the linear, log linear, and polynomial algorithms take no longer than one second. The exponential and factorial algorithms, on the other hand, take more than twelve thousand years! We hope this example can answer the questions proposed in the beginning of this section. It turns out that time complexity does matter and, more importantly, algorithms with low complexity (e.g., linear) can be much faster than those with high complexity (e.g., factorial).

Generally, algorithms with complexity no higher than polynomial are called *scalable algorithms*, whereas ones with complexity higher than polynomial are called *unscalable algorithms*. As shown in fig. 1, unscalable algorithms are not useful in practice since they cannot terminate when the input is only moderately large.

Unfortunately, not all problems have scalable solutions (i.e., can be solved in polynomial time). We call problems that have scalable solutions the *"P" problems*. In this course we are only interested in designing the solutions for the "P" problems. What is more, these solutions should be optimal, that is, they should have the lowest possible time complexity (and preferably, lowest space complexity, as we will see later). The question is, how shall we measure the time complexity? This will be answered in the next section.

# Asymptotic Order of Growth
In section 3 we mentioned that, we do not have to count the number of steps or the run time to measure the time complexity. Alternatively, we can just find its order of growth or, to be more specific, its *asymptotic order of growth*, which provides some kind of bound of the algorithm's time complexity, when the input is large. It is worth noting that asymptotic order of growth does not care about small input. This is because any kind of algorithm, regardless of its time complexity, runs pretty fast given small input. This is shown in the first row of fig. 1, where all algorithms terminate in seconds. This is, however, not the case when the input is (moderately) large. As shown in the last row of fig. 1, linear and log linear algorithms can still finish in seconds while others need days or even years.

Let us come back to the asymptotic order of growth. It provides three kinds of bounds of an algorithm's time complexity: the upper bound (a.k.a., the big $O$), lower bound (the big $\Omega$), and the tight bound (the big $\Theta$). In this course we will only discuss the upper bound and refer the interested reader to [*Lecture Slides for Algorithm Design (Algorithm Analysis)*](http://www.cs.princeton.edu/~wayne/kleinberg-tardos/pdf/02AlgorithmAnalysis.pdf) for the discussion of the other two bounds.

## The big O notation

**Definition 1**. $f(n)$ is $O(g(n))$ if there exist constants $c > 0$ and $n_0 \geq 0$ such that for all $n \geq n_0$ we have 

\begin{equation}
f(n) \leq c g(n).\tag{1}
\end{equation}

Here:
- $n$ is the input size
- $f(n)$ is the time complexity of an algorithm
- $g(n)$ is a function of $n$

As mentioned previously, we only care about the complexity when the input is large. This is why we require eq. (1) to be true when $n \geq n_0$. This is also shown in fig. 2, where $c g(n)$ is always above $f(n)$ when $n \geq n_0$.

<img src="../figure/figure_2.png" width="250" height="200">

Figure 2. The Big O notation. Picture courtesy of [*Introduction to Algorithms (Third Edition)*](https://mitpress.mit.edu/books/introduction-algorithms-third-edition).

## Does the big $O$ really make sense?

The idea of definition 1 is that, we can use $O(g(n))$ to measure the asymptotic order of growth of an algorithm. However, if you think about it, something does not make too much sense. As mentioned in the last sentence of section 4, the whole point of introducing $O(g(n))$ is to measure the time complexity, $f(n)$. The problem is, based on definition 1, to find $g(n)$ (so that we can use it to measure $f(n)$) we need to know $f(n)$. Let me put it this way if this still does not seem strange to you. We need to know $f(n)$ to find $g(n)$ so that we can use $g(n)$ to guess what $f(n)$ looks like. Something is clearly wrong here.

However, using the big $O$ does make sense. To see why this is the case, let us first calculate the time complexity of the function below. This can be done by counting the number of steps in the algorithm.
"""

def fun(n):
    """
    First, print each integer x such that 0 <= x < n
    Second, print the input n 100 times
    
    Parameters
    ----------
    n : a number
    """
    
    for x in range(n):
        print(x)
            
    for i in range(100):
        print(n)

"""Here, we assume that the print function takes one step. Since the first for-loop has $n$ rounds and the second for-loop has 100 rounds, they take $n$ and 100 steps, respectively. As a result, the time complexity of the function, $f(n)$, is

\begin{equation}
f(n) = n + 100.\tag{2}
\end{equation}

We will show that an upper bound of $f(n)$ is $O(n)$. Based on definition 1, all we need to do is to find a number $c > 0$ and a number $n_0 \geq 0$ such that eq. (1) holds for all $n \geq n_0$. Note that if we set $c = 2$ and $n_0 = 100$, we have

\begin{equation}
\underbrace{n + 100}_\text{$f(n)$} \leq \underbrace{2 n}_\text{$c \cdot g(n)$}  \quad \textrm{when} \quad n \geq \underbrace{100}_\text{$n_0$}.\tag{3}
\end{equation}

This proves that $O(n)$ is an upper bound of $f(n)$, which can be written as

\begin{equation}
f(n) = O(n).\tag{4}
\end{equation}

Let us come back to the question we asked in the beginning of this section. That is, why using the big $O$ makes sense? There are three reasons for this.

First, the big $O$ ($O(n)$ in the example above) allows us to focus on the most time consuming steps of an algorithm (the first for-loop) when the input is large and ignore the trivial (less time consuming) steps (the second for-loop).

Second, the big $O$ allows us to group algorithms with different time complexity but the same asymptotic order of growth. For instance, for two algorithms with complexity $n + 100$ and $2 n + 101$, both of them belong to the $O(n)$ (i.e., linear) family (as mentioned in section 4).

Last and most importantly, calculating the big $O$ is usually (much) easier than calculating the time complexity. This seems counterintuitive since definition 1 shows that the big $O$ requires the complexity. However, this is not necessary. In practice, when calculating the big $O$, we can divide the algorithm into several pieces (e.g., based on the logic of the algorithm) and calculate the big $O$ of each piece. In the end, the big $O$ of the algorithm is the highest big $O$ across all pieces. This is called the additivity property of the big $O$ operation.

Take the function in cell 1 for example, we can divide it into two parts: the first and second for-loop. The big $O$ of the first part is $O(n)$ and that of the second is $O(1)$ (since the number of steps in this part is not related to the input or, in other words, is constant). As a result, the big $O$ of the function is the higher one between the two, that is, $O(n)$. This is the same as the one we found previously using the complexity in eq. (4). However, using the big $O$ is easier since we do not have to calculate the exact number of steps ($n$ and 100) in each for-loop.

Hereafter, unless specified otherwise, the time complexity of an algorithm refers to the asymptotic order of growth of the algorithm (using big $O$).

## Exercises
- Implement the functions (indicated by # Implement me)
- Report the time complexity (using the big $O$) of each function (indicated by **Q** and **A**)

### $\star$
"""

def fun_constant(n):
    """
    Print the input number
    
    Parameters
    ----------
    n : an integer
    """
    
    # Implement me
    print(n)

"""**Q**: What is the time complexity of the function?

**A**: The time complexity is $O(1)$.

### $\star$
"""

def fun_linear(n):
    """
    Print each integer x such that 0 <= x <= n
    
    Parameters
    ----------
    n : a number
    
    """
    
    # Implement me
    for x in range(n + 1):
        print(x)

"""**Q**: What is the time complexity of the function?

**A**: The time complexity is $O(n)$.

### $\star\star$
"""

def fun_poly(n):
    """
    For each integer x such that 0 <= x <= n, print each integer y such that 0 <= y <= x 
    
    Parameters
    ----------
    n : a number
    """
    
    # Implement me
    for x in range(n + 1):
        for y in range(x + 1):
            print(y)

"""**Q**: What is the time complexity of the function?

**A**: The time complexity is $O(n^2)$.

### $\star\star\star$
"""

def bubble_sort(arr):
    """
    Bubble sort 
    
    Parameters
    ----------
    arr : a list of number

    Returns
    ----------
    [The list sorted in ascending order, the number of steps]
    """
    
    # The number of steps
    n_steps = 0
    
    arr_temp = list(arr)
    n_steps += len(arr)
    
    for i in range(len(arr_temp)):
        # If the list has been sorted, True by default
        sorted_ = True
        n_steps += 1
        
        for j in range(len(arr_temp) - 1 - i):
            diff = arr_temp[j] - arr_temp[j + 1]
            n_steps += 1

            n_steps += 1
            if diff > 0:
                # Swap
                arr_temp[j], arr_temp[j + 1] = arr_temp[j + 1], arr_temp[j]               
                # The list has not been sorted
                sorted_ = False              
                n_steps += 2

        n_steps += 1
        if sorted_ is True:
            break

    return [arr_temp, n_steps]

# Test
arr_ascend = [1, 2, 3, 4, 5]
arr_descend = [5, 4, 3, 2, 1]

arr_ascend_sorted, n_steps_ascend = bubble_sort(arr_ascend)
arr_descend_sorted, n_steps_descend = bubble_sort(arr_descend)

print('For input list: ' + str(arr_ascend))
print('The sorted list is: ' + str(arr_ascend_sorted))
print('The number of steps is: ' + str(n_steps_ascend), end='\n\n')

print('For input list: ' + str(arr_descend))
print('The sorted list is: ' + str(arr_descend_sorted))
print('The number of steps is: ' + str(n_steps_descend))

"""**Q**: What is the time complexity of the function?

**A**: The time complexity is $O(n^2)$.

# Best and Worst Case Analysis
While analyzing the complexity of fun_constant, fun_linear, and fun_poly is straightforward, doing so for bubble sort is not. As shown in the output, for two input lists with the same elements, the number of steps when the input list is sorted in ascending order (15) is much smaller than that when the input list is sorted in descending order (55). Actually, bubble sort has linear time complexity $O(n)$ when the input is sorted in ascending order, but quadratic complexity $O(n^2)$ when the input is sorted in descending order (can you explain why?). This is shown in figs. 3 and 4.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

def plot(n, order):
    """
    Plot the number of steps bubble sort takes when the input list is sorted in ascending or descending order
    
    Parameters
    ----------
    n : a number
    order : 'Ascending' or 'Descending'
    """
    
    x = [i for i in range(n)]
    y = []

    for i in x:
        if order == 'Ascending':
            arr = [j for j in range(i + 1)]
        else:
            arr = [j for j in range(i, -1, -1)]
            
        arr_soretd, n_steps = bubble_sort(arr)
        y.append(n_steps)
        
    plt.plot(x, y)
    plt.title(order, fontsize=20)
    plt.xlabel('Number of elements', fontsize=20)
    plt.ylabel('Number of steps', fontsize=20)
    plt.xticks(fontsize=20)
    plt.yticks([0, max(y) // 2, max(y)], fontsize=20)
    plt.tight_layout()
    plt.show()

plot(100, 'Ascending')
print("Figure 3. The number of steps bubble sort takes when the input list is sorted in ascending order.")

plot(100, 'Descending')
print("Figure 4. The number of steps bubble sort takes when the input list is sorted in descending order.")

"""Since the time complexity of bubble sort can be either linear or quadratic, which one shall we take as the complexity of the algorithm? The short answer is, the higher one (i.e., the quadratic complexity). We need to introduce two kinds of analysis, named the best and worst case analysis, to get to the long answer.

The *best case analysis* aims to find the lowest possible time complexity of an algorithm. This is done by identifying the complexity with respect to the best possible input (or the best case, as suggested by the name). Best case analysis provides a somewhat optimistic measurement of the complexity. In the bubble sort example, the linear complexity is the best case complexity. This is because when the input list is sorted in ascending order, we only need to pass the list once (can you explain why?).

Conversely, the *worst case analysis* aims to find the highest possible time complexity of an algorithm. It does so by identifying the complexity with respect to the worst possible input (or the worst case). Contrary to the best case analysis, the worst case analysis provides a somewhat pessimistic measurement of the complexity. In the bubble sort example, the quadratic complexity is the worst case complexity. This is because when the input list is sorted in descending order, we need to pass the list $n$ times (where $n$ is the length of the list). In other words, the worst case complexity is $n$ times of the best case one.

While neither the best case nor the worst case complexity is the ideal measurement of the speed of the algorithm, the worst case complexity is usually reported. The idea is simple: we would rather prepare for the worst (and hope for the best) than prepare for the best (and worry about the worst). Hereafter unless otherwise specified the time complexity of an algorithm refers to the worst case complexity. For instance, we will simply say the complexity of bubble sort is $O(n^2)$, although it is actually the worst case complexity of the algorithm.

# Space Complexity
Besides the time complexity there are other kinds of complexities of an algorithm. One of them is the space complexity. Unlike time complexity that talks about the number of steps an algorithm takes, space complexity refers to the number of storage an algorithm uses. Since we always have to store the input, we are more interested in finding the extra space an algorithm requires (to do the work). However, by an abuse of notation, hereafter we will use space complexity to denote the extra space. 

While time complexity is the focus in algorithm design, space complexity is also something we should consider, particularly when two algorithms have the same time complexity but different space complexity. We will analyze the time complexity of all of the algorithms discussed in this course and, for most of them, we will also analyze the space complexity.

It turns out that we can also use the big $O$ to measure space complexity, similar to what we have been doing to measure time complexity. That is, we can divide an algorithm into pieces and measure the big $O$ (in terms of both time and space complexity) for each piece. The big $O$ for the whole algorithm is then the highest big $O$ across all the pieces.

Let us again use the function in cell 1 as an example to show how this can be done. The function can be divided into two pieces: the first and second for-loop. The space complexity for both for-loops are $O(1)$, since they only acquire constant extra space. As a result, the space complexity of the function is the higher one of the two, which in this case is still $O(1)$.

## Exercises
- **Q**: What are the space complexity (using the big $O$) of functions 5.3.1 to 5.3.4?
- **A**: The space complexity of functions 5.3.1 to 5.3.4 are:
    - $O(1)$
    - $O(1)$
    - $O(1)$
    - $O(n)$

# A Good Way to Design Algorithms
As mentioned previously, the goal of algorithm design is to find the optimal solution for a problem. That is, the solution with the lowest time complexity (and preferably, the lowest space complexity). However, for many problems that will be discussed in this course, landing on the optimal solution is not easy. For this reason, instead of aiming for the optimal solution when facing a problem, you are strongly suggested to begin with a brute-force solution, something although inefficient (in terms of the time and space complexity) but indeed does the job. After that, you can keep improving this brute-force solution until reaching the optimal one (or calling it a day).

To give you a taste of what this journey looks like, we will look at a famous problem, finding the $n$th number in the [Fibonacci series](https://en.wikipedia.org/wiki/Fibonacci_number). We will provide the brute-force solution and you should improve it until reaching the optimal solution. For now, it is absolutely fine if you cannot make any improvement on the brute-force solution. However, at least try. This will help you to think as a computer scientist.

## The brute-force solution
- exponential time complexity
- linear space complexity
"""

def fun_brute_force(n):
    """
    The brute-force (recursive) solution for Fibonacci series.
    The solution has exponential time complexity and linear space complexity.
    
    Parameters
    ----------
    n : a number
    
    Returns
    ----------
    The nth number in Fibonacci series
    """
        
    # Base
    if n <= 1:
        return n
    
    # Recursion
    return fun_brute_force(n - 2) + fun_brute_force(n - 1)

"""Analyzing the time and space complexity of the brute-force solution is not straightforward, since it is a recursive function (a function that calls itself). One way to do this is to draw the recursion tree. Fig. 5 illustrates the recursion tree when $n$ is 5. It shows that level 0 (the top level) has only 1 node (f(5)), level 1 has 2 nodes, and level 2 has 4 nodes. In other words, the number of nodes is doubled when moving down to the next level. We can prove that for the $i$th level, the maximum number of node is $2^i$. Since the recursion tree has $n$ levels (from level 0 to $n - 1$), the total number of nodes in the tree is $2^n - 1$. Since we have to calculate the value of each node, the time complexity of the algorithm is $2^n$, or exponential.

<img src="../figure/figure_5.pdf" width="400" height="400">

Figure 5. The recursion tree when $n = 5$.

Here we will experimentally demonstrate what we theoretically explained. That is, the time complexity of the brute-force solution is exponential. While we could measure the exact number of steps, as what we did in the bubble sort example, here we will measure the run time of the algorithm across different inputs (from 0 to 29). As shown in fig. 6., the run time increases exponentially, echoing the exponential time complexity.
"""

import time

def plot(n, fun):
    """
    Plot the run time of a function.
    
    Parameters
    ----------
    n : a number
    function : a function
    """
    
    x = [i for i in range(n)]
    y = []

    for i in x:
        start = time.time()
        fun(i)
        end = time.time()
        y.append(end - start)
        
    plt.plot(x, y)
    plt.xlabel('$n$', fontsize=20)
    plt.ylabel('Run time', fontsize=20)
    plt.xticks(fontsize=20)
    plt.yticks([0, max(y) // 2, max(y)], fontsize=20)
    plt.tight_layout()
    plt.show()

plot(30, fun_brute_force)
print("Figure 6. The run time of the brute-force solution.")

"""It turns out that analyzing the space complexity is even tricker, since it involves the mechanism of recursion (which is not the focus of this course). Here we will just provide an oversimplified analysis and leave the detailed discussion to Chapter 2. In simple words, the space complexity (of the brute-force solution) is proportional to the number of levels in the recursion tree (a.k.a., the height of the tree). Since the tree has $n$ levels (as shown in fig. 5), the space complexity is $O(n)$.

## Exercise (an improved solution $\star\star$)
- Implement the functions (indicated by # Implement me)
- The function should have:
    - linear time complexity
    - linear space complexity
"""

def fun_dp_linear(n):
    """
    An improved solution for Fibonacci series.
    The solution has linear time complexity and linear space complexity.
    
    Parameters
    ----------
    n : a number
    
    Returns
    ----------
    The nth number in Fibonacci series
    
    """
    
    # Implement me
    if n <= 1:
        return n
    
    arr = [0] * (n + 1)
    arr[1] = 1
    
    for i in range(2, n + 1):
        arr[i] = arr[i - 2] + arr[i - 1]
        
    return arr[n]

"""The logic of the improved solution is easy. When using a list to store the numbers, the $i$th (where $i \geq 2$) number in the list is the sum of the $i - 2$th number and the $i - 1$th number. What is not so obvious is that, the improved solution is designed using an approach named Dynamic Programming. This is one of the key methods in algorithm design and we will use roughly two weeks to talk about it (in Chapter 6). For now, let us ignore how it works and focus on the time complexity. Since we only have to pass the list once (indicated by the for-loop), the time complexity is $O(n)$, or linear. This is also illustrated in fig. 7."""

plot(300, fun_dp_linear)
print("Figure 7. The run time of the improved solution.")

"""## Exercise (the optimal solution $\star\star\star$)
- Implement the functions (indicated by # Implement me)
- The function should have:
    - linear time complexity
    - constant space complexity
"""

def fun_dp_constant(n):
    """
    The optimal solution for Fibonacci series.
    The solution has linear time complexity and constant space complexity.
    
    Parameters
    ----------
    n : a number
    
    Returns
    ----------
    The nth number in Fibonacci series
    
    """
    
    # Implement me
    if n <= 1:
        return n
    
    a, b = 0, 1
        
    for i in range(2, n + 1):
        c = a + b
        a = b
        b = c

    return c

"""Compared to the improved solution with linear space complexity, here the space complexity is constant. The idea is that, since we only need the previous two ($i - 2$th and $i - 1$th) number to calculate the current ($i$th) number, we do not need a list to store all the other previous numbers. Instead, we can simply use three variables to represent the three numbers. This brings the space complexity down to constant, since the size of the space is not related to the input. As a matter of fact, we cannot find a solution with lower time or space complexity (can you explain why?), which makes the solution optimal. Fig. 8 shows the linear growth of the running time of the optimal solution."""

plot(300, fun_dp_constant)
print("Figure 8. The run time of the optimal solution.")

import time
start_time=time.time()
for j in range(2,1000):
  j=2*j
  for k in rnage(j,1000):
    k=k**2
end_time=time.time()
used_time=end_time=start_time
print(used_time)