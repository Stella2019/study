# -*- coding: utf-8 -*-
"""chapter_3_binary_search_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fc4-cKdXy7Xwm1cz3iYK3MOnXIzwZwid

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Overview" data-toc-modified-id="Overview-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href="#Learning-Objectives" data-toc-modified-id="Learning-Objectives-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Learning Objectives</a></span></li><li><span><a href="#Binary-Search" data-toc-modified-id="Binary-Search-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Binary Search</a></span><ul class="toc-item"><li><span><a href="#Example-($\star$)" data-toc-modified-id="Example-($\star$)-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span>Example ($\star$)</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.2"><span class="toc-item-num">3.2&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#Exercise-($\star$)" data-toc-modified-id="Exercise-($\star$)-3.3"><span class="toc-item-num">3.3&nbsp;&nbsp;</span>Exercise ($\star$)</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.4"><span class="toc-item-num">3.4&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#The-idea-of-binary-search" data-toc-modified-id="The-idea-of-binary-search-3.5"><span class="toc-item-num">3.5&nbsp;&nbsp;</span>The idea of binary search</a></span><ul class="toc-item"><li><span><a href="#Why-does-binary-search-work?" data-toc-modified-id="Why-does-binary-search-work?-3.5.1"><span class="toc-item-num">3.5.1&nbsp;&nbsp;</span>Why does binary search work?</a></span></li><li><span><a href="#Why-does-binary-search-matter?" data-toc-modified-id="Why-does-binary-search-matter?-3.5.2"><span class="toc-item-num">3.5.2&nbsp;&nbsp;</span>Why does binary search matter?</a></span><ul class="toc-item"><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.5.2.1"><span class="toc-item-num">3.5.2.1&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li><li><span><a href="#How-is-binary-search-implemented?" data-toc-modified-id="How-is-binary-search-implemented?-3.5.3"><span class="toc-item-num">3.5.3&nbsp;&nbsp;</span>How is binary search implemented?</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.5.4"><span class="toc-item-num">3.5.4&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li><li><span><a href="#Exercise-($\star\star$)" data-toc-modified-id="Exercise-($\star\star$)-3.6"><span class="toc-item-num">3.6&nbsp;&nbsp;</span>Exercise ($\star\star$)</a></span></li><li><span><a href="#Exercise-($\star\star$)" data-toc-modified-id="Exercise-($\star\star$)-3.7"><span class="toc-item-num">3.7&nbsp;&nbsp;</span>Exercise ($\star\star$)</a></span></li><li><span><a href="#Exercise-($\star\star$)" data-toc-modified-id="Exercise-($\star\star$)-3.8"><span class="toc-item-num">3.8&nbsp;&nbsp;</span>Exercise ($\star\star$)</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.9"><span class="toc-item-num">3.9&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#Exercise-($\star$)" data-toc-modified-id="Exercise-($\star$)-3.10"><span class="toc-item-num">3.10&nbsp;&nbsp;</span>Exercise ($\star$)</a></span></li><li><span><a href="#Exercise-($\star\star$)" data-toc-modified-id="Exercise-($\star\star$)-3.11"><span class="toc-item-num">3.11&nbsp;&nbsp;</span>Exercise ($\star\star$)</a></span></li><li><span><a href="#Exercise-($\star$)" data-toc-modified-id="Exercise-($\star$)-3.12"><span class="toc-item-num">3.12&nbsp;&nbsp;</span>Exercise ($\star$)</a></span></li><li><span><a href="#Exercise-($\star\star$)" data-toc-modified-id="Exercise-($\star\star$)-3.13"><span class="toc-item-num">3.13&nbsp;&nbsp;</span>Exercise ($\star\star$)</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.14"><span class="toc-item-num">3.14&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href="#Exercise-($\star\star$)" data-toc-modified-id="Exercise-($\star\star$)-3.15"><span class="toc-item-num">3.15&nbsp;&nbsp;</span>Exercise ($\star\star$)</a></span></li><li><span><a href="#Discussion" data-toc-modified-id="Discussion-3.16"><span class="toc-item-num">3.16&nbsp;&nbsp;</span>Discussion</a></span></li></ul></li><li><span><a href="#The-Master-Theorem" data-toc-modified-id="The-Master-Theorem-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>The Master Theorem</a></span><ul class="toc-item"><li><span><a href="#Exercise" data-toc-modified-id="Exercise-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>Exercise</a></span></li><li><span><a href="#Exercise" data-toc-modified-id="Exercise-4.2"><span class="toc-item-num">4.2&nbsp;&nbsp;</span>Exercise</a></span></li><li><span><a href="#Exercise" data-toc-modified-id="Exercise-4.3"><span class="toc-item-num">4.3&nbsp;&nbsp;</span>Exercise</a></span></li><li><span><a href="#Exercise" data-toc-modified-id="Exercise-4.4"><span class="toc-item-num">4.4&nbsp;&nbsp;</span>Exercise</a></span></li></ul></li><li><span><a href="#Appendix" data-toc-modified-id="Appendix-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Appendix</a></span><ul class="toc-item"><li><span><a href="#Hints-for-exercise-3.3:" data-toc-modified-id="Hints-for-exercise-3.3:-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>Hints for <a href="#3.3">exercise 3.3</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.6:" data-toc-modified-id="Hints-for-exercise-3.6:-5.2"><span class="toc-item-num">5.2&nbsp;&nbsp;</span>Hints for <a href="#3.6">exercise 3.6</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.7:" data-toc-modified-id="Hints-for-exercise-3.7:-5.3"><span class="toc-item-num">5.3&nbsp;&nbsp;</span>Hints for <a href="#3.7">exercise 3.7</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.8:" data-toc-modified-id="Hints-for-exercise-3.8:-5.4"><span class="toc-item-num">5.4&nbsp;&nbsp;</span>Hints for <a href="#3.8">exercise 3.8</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.10:" data-toc-modified-id="Hints-for-exercise-3.10:-5.5"><span class="toc-item-num">5.5&nbsp;&nbsp;</span>Hints for <a href="#3.10">exercise 3.10</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.11:" data-toc-modified-id="Hints-for-exercise-3.11:-5.6"><span class="toc-item-num">5.6&nbsp;&nbsp;</span>Hints for <a href="#3.11">exercise 3.11</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.12:" data-toc-modified-id="Hints-for-exercise-3.12:-5.7"><span class="toc-item-num">5.7&nbsp;&nbsp;</span>Hints for <a href="#3.12">exercise 3.12</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.13:" data-toc-modified-id="Hints-for-exercise-3.13:-5.8"><span class="toc-item-num">5.8&nbsp;&nbsp;</span>Hints for <a href="#3.13">exercise 3.13</a>:</a></span></li><li><span><a href="#Hints-for-exercise-3.15:" data-toc-modified-id="Hints-for-exercise-3.15:-5.9"><span class="toc-item-num">5.9&nbsp;&nbsp;</span>Hints for <a href="#3.15">exercise 3.15</a>:</a></span></li></ul></li></ul></div>

<b>

<p>
<center>
<font size="5">
Lecture Note for Computer Science Foundations (DATS 6450)
</font>
</center>
</p>

<p>
<center>
<font size="4">
Chapter 3: Binary Search (Solution)
</font>
</center>
</p>

<p>
<center>
<font size="3">
Data Science, Columbian College of Arts & Sciences, George Washington University
</font>
</center>
</p>

<p>
<center>
<font size="3">
Author: Yuxiao Huang
</font>
</center>
</p>

</b>

# Overview
- We will discuss two topics here:
    - one is a search algorithm, named *Binary Search*, which is one of the most widely used algorithm for searching
    - the other is a theorem, named the *Master Theorem*, which allows us to *read* the time complexity of a recursive algorithm from the recursive definition of the complexity
- Particularly, the discussion of binary search can be divided into two parts:
    - theory, where we will describe the idea of the algorithm
    - coding
        - where (most of the time) we will start with some examples and then work on some exercises
        - particularly, the examples and exercises are organized in such a way that, an exercise (most of the time) is a follow-up on some example prior to it
        - **you should analyze the (time and space) complexity of each example and exercise**
- We will use stars to represent the difficulty of the exercises:
    - $\star$ means very easy
    - $\star\star$ means easy
    - $\star\star\star$ means medium
    - $\star\star\star\star$ means difficult
    - $\star\star\star\star\star$ means very difficult

# Learning Objectives
Students should know:
- how binary search works
- how to apply binary search to design optimal solutions (ones with the lowest complexity)
- how to apply the master theorem for analyzing the time complexity of a recursive algorithm

# Binary Search
Instead of introducing the algorithm right away, let us begin with something easy, that is, finding a target in an array.

<a id='3.1'></a>

## Example ($\star$)
- Problem:
    - find a target in an array
    - specifically, the solution returns
        - the index of the target, if the target is in the array
        - None, otherwise
    - you can assume the items in the array are unique
    - the solution has the complexity below
    - this example will be followed by
        - [exercise 3.3](#3.3)
- Complexity:
    - $O(n)$ time
    - $O(1)$ space
- Skills: 
    - solving a problem by starting with a brute-force solution
- Logic: loop over the array and record the index of the target, if there is any
"""

# Implementation
def fun_31(arr, target):
    """
    Find a target in an array
    You can assume the items in the array are unique
    
    Parameters
    ----------
    arr : a list of integers
    target : an integer
    
    Returns
    ----------
    The index of the target, if the target is in the array
    None, otherwise
    """
    
    # Initialize idx with None (which should be returned if the target is not in the array)
    idx = None
    
    # Loop over each element in the array
    for i in range(len(arr)):
        if arr[i] == target:
            # Update idx
            idx = i
            
    return idx

# Test
arr_1 = [2]
arr_2 = [3]
arr_3 = [2, 3]

print(fun_31(arr_1, 3))
print(fun_31(arr_2, 3))
print(fun_31(arr_3, 3))

"""## Discussion
**Q**: Does fun_31 work when the input array is empty?

**A**: It does since when the input array is empty (i.e., len(arr) is 0), the for-loop will not be executed.

<a id='3.3'></a>

## Exercise ($\star$)
- Problem:
    - follow up on [example 3.1](#3.1)
    - improve the speed of [example 3.1](#3.1)
    - find the solution with the complexity below
    - this exercise will be followed by
        - [exercise 3.5](#3.5)
- Complexity:
    - $O(n)$ time
    - $O(1)$ space
- Skills: 
    - avoid unnecessary computation
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.3)
"""

# Implementation
def fun_33(arr, target):
    """
    Find a target in an array
    You can assume the items in the array are unique
    
    Parameters
    ----------
    arr : a list of integers
    target : an integer
    
    Returns
    ----------
    The index of the target, if the target is in the array
    None, otherwise
    """
    
    # Implemet me
    # Loop over each element in the array
    for i in range(len(arr)):
        if arr[i] == target:
            return i
            
    return None

# Test
arr_1 = [2]
arr_2 = [3]
arr_3 = [2, 3]

print(fun_31(arr_1, 3))
print(fun_31(arr_2, 3))
print(fun_31(arr_3, 3))

"""## Discussion
The reason why fun_33 is faster than fun_31 is that, fun_33 returns the index as soon as it finds the target, without looping over the remaining numbers in the array (as fun_31 does), thus avoiding unnecessary computation. This is shown in fig. 1, where the two functions search for a target randomly selected from the array.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import time
# %matplotlib inline
import matplotlib.pyplot as plt

def plot(n, funs):
    """
    Plot the run time of the functions (with respect to the input size).
    
    Parameters
    ----------
    n : an integer
    funs : a list of functions
    """
    
    np.random.seed(0)
    
    x = list(range(1, n + 1))
    ys = [[] for _ in range(len(funs))]

    for i in x:
        target = np.random.randint(low=1, high=i + 1)
        for j in range(len(funs)):
            start = time.time()
            funs[j](list(range(1, i + 1)), target)
            end = time.time()
            ys[j].append(end - start)
    
    for j in range(len(funs)):
        plt.plot(x, ys[j], label=funs[j].__name__)
    plt.xlabel('$n$', fontsize=20)
    plt.ylabel('Run time', fontsize=20)
    plt.xticks([min(x), max(x)], fontsize=20)
    plt.yticks([min([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))]), max([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))])], fontsize=20)
    plt.legend(fontsize=20)
    plt.tight_layout()
    plt.show()

plot(10 ** 3, [fun_31, fun_33])
print("Figure 1. The run time of fun_31 and fun_33.")

"""## The idea of binary search
While fun_33 is faster than fun_31, both of them have $O(n)$ time complexity. This is because (in the worst case) they have to pass the whole array to find the target. However, the claim above is only true when the array is not sorted. Because when this is the case, as long as there is still one element in the array that has not been visited, we cannot conclude that the target is not in the array (since it can be the unvisited element). However, things will be quite different when the array is sorted (can you explain why?).

Specifically, assume the array has been sorted in ascending order. Then instead of starting the search from the first element of the array (as what we did in fun_31 and fun_33), we begin with the middle element. There are three possible scenarios in terms of the difference between the middle one and the target:
- if middle = target, we return the index of the middle element
- if middle > target, we first discard the whole elements behind the middle one, and then keep looking for the target in the elements in front of the middle one
- if middle < target, we first discard the whole elements in front of the middle one, and then keep looking for the target in the elements behind the middle one

The steps above illustrate the idea of *Binary Search*. The reason why there is a "binary" in the name is that, every time we only need to look at one half of an array (as mentioned in the last two steps). In the following sections, we will explain why binary search works, why it matters, and more importantly, how it is implemented.

### Why does binary search work?
It is not difficult to figure out why the last two steps in section 3.5 (where the middle is different from the target) work. Take the second step for example, because the middle is larger than the target, all the elements behind the middle item will also be larger than the target (since the array is sorted in ascending order). As a result, we only need to focus on the smaller items in front of the middle item, one of which could be the same as the target. For similar reason, we can only focus on the larger items behind the middle item when it is smaller than the target (as what we did in the last step in section 3.5).

### Why does binary search matter?
The question is, why discarding half of the array each time matters? The short answer is, because this makes binary search much faster than algorithms such as fun_33. This is a big deal since, as we discussed previously, the time complexity of fun_33 is $O(n)$ (in other words, it is already pretty fast). Being faster than linear algorithms makes binary search one of the most widely used searching algorithms in practice.

The long answer, on the other hand, relies on the analysis of the time complexity of binary search. Here we will use two ways to discuss the complexity. The first one is based on a data structure, named *Binary Search Tree* (BST), shown in fig. 2 (we will give the formal definition of BST in chapter 5). The second one is based on a theorem, named the *Master Theorem*, which will be discussed in section 4.

<img src="../figure/figure_2.pdf" width="800" height="800">

Figure 2. The Binary Search Tree (BST) for input array [0, $\ldots$, 9].

The BST in fig. 2 illustrates the steps in binary search for input array [0, $\ldots$, 9]. Particularly, each node in the tree contains three numbers. The first number (the one outside the parentheses) is the index of the middle item of the array under consideration, and the last two numbers (the ones in the parentheses) are the indices where the array starts and ends. More formally, if the array we are working with starts at index *left* and ends at index *right*, then the index of the middle item, *mid*, can be calculated as

\begin{equation}
mid = (left + right) \ // \ 2,\tag{1}
\end{equation}
where // is the floor division (a.k.a., integer division). For instance, the input array used for fig. (2) starts at index 0 (i.e., left = 0) and ends at index 9 (right = 9). This is why the first number in the root of the BST is 4 (mid = 4), which was calculated using eq. (1).

However, while eq. (1) is correct in theory, it could be problematic in practice. Specifically, it may result in [Integer Overflow](https://en.wikipedia.org/wiki/Integer_overflow). Here is an oversimplified example illustrating the idea. Assume somewhere in the binary search, the subarray (obtained by discarding half of the input array) starts at index $inf$ // 2 + 1 and ends at index $inf$ (where $inf$ is the maximum integer allowed by Python). That is,

\begin{equation}
left = inf\ //\ 2 + 1 \quad \textrm{and} \quad right = inf. \tag{2}
\end{equation}

When calculating mid using eq. (1), the numerator of the ratio can be written as

\begin{equation}
left + right = inf\ //\ 2 + 1 + inf, \tag{3}
\end{equation}
which, unfortunately, is larger than $inf$, in turn causing overflow.

This problem can be handled by rewriting eq. (1) as

\begin{equation}
mid = left + (right - left) \ // \ 2.\tag{4}
\end{equation}

This equation still gives us the mid (between left and right) without causing overflow (can you explain why?).

After introducing the meaning of each number in a node of the BST, let us discuss the relationship between the numbers in neighboring nodes. For two nodes connected by an edge, we usually call a node a *parent* if the edge comes out of it, and call a node a *child* if the edge goes into it. It turns out that, in BST the left (second number in the node) of a left child is the sames as the left of the parent, while the right (third number) of the child is the mid of the parent minus 1. In other words, the left child represents the left subarray of the array denoted by its parent. The left (second number) of a right child, on the other hand, is the mid of the parent plus 1, while the right (third number) of the child is the same as the right of the parent. In other words, the right child represents the right subarray of the array denoted by its parent.

Now let us use the BST in fig. 2 to illustrate the three steps of binary search (mentioned in section 3.5). We first compare the middle item (whose index is the first number in the root) with the target. Then:
- if middle == target, we return the index of the middle item
- if middle > target, we go to the left subtree (can you explain why?)
- if middle < target, we go to the right subtree

We repeat the above steps until the target has been found or we have reached a leaf node (where left, mid, and right are the same) and have nowhere to go.

Here is an example illustrating the steps for finding target 6 in the array.
- First, we compare the target (6) with the middle item of the input array (4). Since the target is larger than the item, we go to the right subtree.
- Next, we compare 6 with the middle item of the subarray (7). Since the target is smaller than the item, we go to the left subtree.
- Next, we compare 6 with the middle item of the subarray (5). Since the target is larger than the item, we go to the right subtree.
- Last, we compare 6 with the middle item of the subarray (6). Since they are equal, we return the index, 6.

The above steps are also shown by the red path in the figure below.

<img src="../figure/figure_3.pdf" width="800" height="800">

Figure 3. The steps (denoted by the red path) for finding target 6 in input array [0, $\ldots$, 9].

#### Discussion
- **Q**: Can you explain what happens when the target is 0?
- **Q**: Can you explain what happens when the target is 10?

The example and discussion above show that, no matter what the target is, binary search can always terminate within 4 steps. Here 4 is the number of nodes in the longest path (from the root to a leaf) in fig. 2, which is also the height of the BST minus 1. This makes sense since we will definitely know whether the target is in the array (or not) after walking through one of the paths, whose length cannot be larger than 4. 

The finding above is interesting because it reveals the time complexity of binary search, that is, $O(height)$. The question is, what is the value of this mysterious number, *height*, when there are $n$ elements in the input array? To answer this question, let us briefly review what happens in binary search, or even better, in the BST in fig. 2. In general, we iteratively divide the input array in half until the subarray only contains one element. In other words, we repeatedly divide $n$ (the length of the input array) by 2 until $n$ becomes 1. The number of iteration, $k$, can be calculated by solving the equation below

\begin{equation}
2 ^ k = n,\tag{2}
\end{equation}
which gives us
\begin{equation}
k = \log(n).\tag{3}
\end{equation}

That is, the height of a BST for input array with size $n$ is $\log(n)$. As a result, the time complexity of binary search is $O(\log(n))$. In section 4, we will use the Master Theorem to read the complexity of the binary search from the recursive definition of the complexity.

The above complexity theoretically explains the question we asked in the beginning of this section, that is, why binary search matters. It is simply because a $O(\log(n))$ algorithm is much faster than a $O(n)$ one (we will see this quite often later). This is the key reason why binary search is one of the most widely used algorithms in reality.

<a id='3.5.3'></a>

### How is binary search implemented?
The implementation of binary search is straightforward. Basically we can simply follow the steps in section 3.5 to implement the algorithm. Below is one way to do so. This implementation will be followed by
- [exercise 3.6](#3.6)
- [exercise 3.12](#3.12)
"""

def binary_search(arr, target):
    """
    The binary search algorithm
    
    Parameters
    ----------
    arr : a list of integers
    target : an integer
    
    Returns
    ----------
    The index of the target, if the target is in the array
    None, otherwise
    """
    
    # The starting and ending point of the input array
    left, right = 0, len(arr) - 1
    
    # While the subarray is not empty
    while (left <= right):
        # Get the index of the middle item in the subarray
        mid = left + (right - left) // 2
        
        # If the middle item equals the target
        if arr[mid] == target:
            return mid
        # If the middle item is larger than the target
        elif arr[mid] > target:
            right = mid - 1
        # If the middle item is smaller than the target
        else:
            left = mid + 1
            
    return None

# Test
arr_1 = [2]
arr_2 = [3]
arr_3 = [2, 3]

print(binary_search(arr_1, 3))
print(binary_search(arr_2, 3))
print(binary_search(arr_3, 3))

"""### Discussion
The difference (in running time) between binary search and fun_33 is shown in fig. 4. This experimentally illustrates what we theoretically explained, that is, binary search (with $O(\log(n))$ time complexity) is indeed faster than the linear algorithms (with $O(n)$ complexity).
"""

plot(10 ** 4, [fun_33, binary_search])
print("Figure 4. The run time of fun_33 and binary search.")

"""<a id='3.6'></a>

## Exercise ($\star\star$)
- Problem:
    - follow up on [section 3.5.3](#3.5.3)
    - unlike in binary search where we return None when the target is not in the input array, here we return the index where the target should be inserted (so that the array is still sorted in ascending order)
    - see examples in the testing cases
    - you can assume the items in the array are unique
    - find the solution with the complexity below
- Complexity:
    - $O(\log(n))$ time
    - $O(1)$ space
- Skills: 
    - tweaking an existing solution to fulfill the new requirements
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.6)
"""

# Implementation
def fun_36(arr, target):
    """
    Find a target in an array
    You can assume the items in the array are unique
    
    Parameters
    ----------
    arr : a list of integers
    num : an integer
    
    Returns
    ----------
    The index of the target, if the target is in the array
    The index where the target should be inserted (so that the array is still sorted in ascending order), otherwise
    """
    
    # Implement me
    # The starting and ending point of the input array
    left, right = 0, len(arr) - 1
    
    # While the subarray is not empty
    while (left <= right):
        # Get the index of the middle item in the subarray
        mid = left + (right - left) // 2
        
        # If the middle item equals the target
        if arr[mid] == target:
            return mid
        # If the middle item is larger than the target
        elif arr[mid] > target:
            right = mid - 1
        # If the middle item is smaller than the target
        else:
            left = mid + 1
            
    return left

# Test
arr_1 = [2]
arr_2 = [4]
arr_3 = [2, 3, 4]

print(fun_36(arr_1, 3))
print(fun_36(arr_2, 3))
print(fun_36(arr_3, 3))

"""<a id='3.7'></a>

## Exercise ($\star\star$)
- Problem:
    - check whether the target is a square of another number
    - find the solution with the complexity below
    - this exercise will be followed by
        - [exercise 3.8](#3.8)
- Complexity:
    - $O(n)$ time
    - $O(1)$ space
- Skills: 
    - handling corner cases
    - handling overflow
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.7)
"""

# Implementation
def fun_37(target):
    """
    Check whether the target is a square of another number
    
    Parameters
    ----------
    target : an integer
    
    Returns
    ----------
    True, if the target is a square of another number
    False, otherwise
    """
    
    # Implement me
    # Corner case
    if target == 0:
        return True
    
    # For each target i
    for i in range(1, target + 1):
        if i == target / i:
            return True

    return False

# Test
target_1 = -1
target_2 = 0
target_3 = 5
target_4 = 9
target_5 = 100

print(fun_37(target_1))
print(fun_37(target_2))
print(fun_37(target_3))
print(fun_37(target_4))
print(fun_37(target_5))

"""<a id='3.8'></a>

## Exercise ($\star\star$)
- Problem:
    - follow up on [exercise 3.7](#3.7)
    - find the solution with the complexity below
- Complexity:
    - $O(\log(n))$ time
    - $O(1)$ space
- Skills: 
    - tweaking an existing solution to fulfill the new requirements
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.8)
"""

# Implementation
def fun_38(target):
    """
    Check whether the target is a square of another number
    
    Parameters
    ----------
    target : an integer
    
    Returns
    ----------
    True, if the target is a square of another number
    False, otherwise
    """
    
    # Implement me
    # Corner case
    if target == 0:
        return True
    
    # The starting and ending point of a range
    left, right = 1, target

    # While the range is not empty
    while left <= right:
        # Get the middle item in the range
        mid = left + (right - left) // 2
        div = target / mid

        # If the middle item equals the square root of the target
        if mid == div:
            return True
        # If the middle item is larger than the square root of the target
        elif mid > div:
            right = mid - 1
        # If the middle item is smaller than the square root of the target
        else:
            left = mid + 1

    return False

# Test
target_1 = -1
target_2 = 0
target_3 = 5
target_4 = 9
target_5 = 100

print(fun_38(target_1))
print(fun_38(target_2))
print(fun_38(target_3))
print(fun_38(target_4))
print(fun_38(target_5))

"""## Discussion
The difference (in running time) between fun_37 (the linear solution) and fun_38 (the binary search) can be seen in fig. 5. This illustrates the advantage of algorithms (e.g., fun_38) with log time complexity over ones (e.g., fun_37) with linear time complexity.
"""

def plot(n, funs):
    """
    Plot the run time of the functions (with respect to the input size).
    
    Parameters
    ----------
    n : an integer
    funs : a list of functions
    """
        
    x = list(range(n + 1))
    ys = [[] for _ in range(len(funs))]

    for i in x:
        for j in range(len(funs)):
            start = time.time()
            funs[j](i)
            end = time.time()
            ys[j].append(end - start)
    
    for j in range(len(funs)):
        plt.plot(x, ys[j], label=funs[j].__name__)
    plt.xlabel('$n$', fontsize=20)
    plt.ylabel('Run time', fontsize=20)
    plt.xticks([min(x), max(x)], fontsize=20)
    plt.yticks([min([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))]), max([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))])], fontsize=20)
    plt.legend(fontsize=20)
    plt.tight_layout()
    plt.show()

plot(10 ** 4, [fun_37, fun_38])
print("Figure 5. The run time of fun_37 and fun_38.")

"""<a id='3.10'></a>

## Exercise ($\star$)
- Problem:
    - assume I randomly picked a number between 1 and $n$
    - your job is to find which number I picked, in no more than $n$ attempts
    - particularly, for each of your attempt, a helper function (named, well, helper) will return one of the two numbers:
        - 1, if your guess = the number I picked
        - 0, if your guess $\neq$ the number I picked
    - find the solution with the complexity below
    - for the spirit of a game, the testing result was hidden intentionally
    - this exercise will be followed by
        - [exercise 3.11](#3.11)
- Complexity:
    - $O(n)$ time
    - $O(1)$ space
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.10)
"""

# Implementation
def fun_310(n):
    """
    Find the number I picked (between 1 and n), in no more than n attempts
    
    Parameters
    ----------
    n : a positive integer
    
    Returns
    ----------
    The target I picked (between 1 and n)
    """
    
    # Implement me
    # For each number i between 1 and n
    for i in range(1, n + 1):
        if helper(i, n) == 1:
            return i
        
    return None

def helper(i, n):
    """
    Check whether the number I picked, i, is the same as the real one, n
    
    Parameters
    ----------
    i : a positive integer
    n : a positive integer
    
    Returns
    ----------
    1, if i = n
    0, otherwise
    """
    
    # Randomly pick a number between 1 and n
    np.random.seed(0)
    num = np.random.randint(low=1, high=n + 1)
    
    return 1 if i == num else 0

# Test
n_1 = 1
n_2 = 3
n_3 = 100

print(fun_310(n_1))
print(fun_310(n_2))
print(fun_310(n_3))

"""<a id='3.11'></a>

## Exercise ($\star\star$)
- Problem:
    - follow up on [exercise 3.10](#3.10)
    - in this case, you will work with a helper function that is a little bit more powerful
    - particularly, for each of your attempt, the helper function will return one of the three numbers:
        - 0,  if your guess = the number I picked
        - -1, if your guess > the number I picked (indicating your guess should go down)
        - 1,  if your guess < the number I picked (indicating your guess should go up)
    - your job is to find which number I picked, in no more than $\log(n)$ attempts
    - find the solution with the complexity below
    - for the spirit of a game, the testing result was hidden intentionally
- Complexity:
    - $O(\log(n))$ time
    - $O(1)$ space
- Skills: 
    - tweaking an existing solution to fulfill the new requirements
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.11)
"""

# Implementation
def fun_311(n):
    """
    Find the number I picked (between 1 and n), in no more than \log(n) attempts
    
    Parameters
    ----------
    n : a positive integer
    
    Returns
    ----------
    The number I picked (between 1 and n)
    """
    
    # Implement me
    # The starting and ending point of a range
    left, right = 1, n

    # While the range is not empty
    while left <= right:
        # Get the middle item in the range
        mid = left + (right - left) // 2
        # Get the result of helper
        num = helper(mid, n)

        # If the middle item equals the target I picked
        if num == 0:
            return mid
        # If the middle item is larger than the target I picked
        elif num == -1:
            right = mid - 1
        # If the middle item is smaller than the target I picked
        else:
            left = mid + 1

    return left

def helper(i, n):
    """
    Check whether the number I picked, i, is the same as the real one, n
    
    Parameters
    ----------
    i : a positive integer
    n : a positive integer
    
    Returns
    ----------
    0,  if i = n
    -1, if i > n
    1,  if i < n
    """
    
    # Randomly pick a number between 1 and n
    np.random.seed(1)
    num = np.random.randint(low=1, high=n + 1)
    
    if i == num:
        return 0
    if i > num:
        return -1
    if i < num:
        return 1

# Test
n_1 = 1
n_2 = 3
n_3 = 100

print(fun_311(n_1))
print(fun_311(n_2))
print(fun_311(n_3))

"""<a id='3.12'></a>

## Exercise ($\star$)
- Problem:
    - follow up on [section 3.5.3](#3.5.3)
    - in this case, you should find a target in a ($m \times n$) matrix
    - find the solution with the complexity below
    - this exercise will be followed by
        - [exercise 3.13](#3.13)
- Complexity:
    - $O(m \times n)$ time
    - $O(1)$ space
- Skills: 
    - handling corner cases
    - looping over a matrix
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.12)
"""

# Implementation
def fun_312(matrix, target):
    """
    Find a target in a (m * n) matrix
    
    Parameters
    ----------
    matrix : list[list[int]] (i.e., a list of list of integers)
    target : an integer
    
    Returns
    ----------
    [i, j], if matrix[i, j] = target 
    None, if the target is not in the matrix
    """
    
    # Implement me
    # Corner case
    # If matrix is [] (an empty list) or [[]] (a list of an empty list)
    if len(matrix) == 0 or len(matrix[0]) == 0:
        return None
    
    # Get the number of rows and columns
    m, n = len(matrix), len(matrix[0])
    
    # For each row
    for i in range(m):
        # For each column
        for j in range(n):
            if matrix[i][j] == target:
                return [i, j]
            
    return None

# Test
matrix_1 = []
target_1 = 2

matrix_2 = [[]]
target_2 = 2

matrix_3 = [[1], [2]]
target_3 = 2

matrix_4 = [[1,  2,  3,  4],
            [5,  6,  7,  8],
            [9, 10, 11, 12]]
target_4 = 8

matrix_5 = [[1,  2,  3,  4],
            [5,  6,  7,  8],
            [9, 10, 11, 12]]
target_5 = 13

print(fun_312(matrix_1, target_1))
print(fun_312(matrix_2, target_2))
print(fun_312(matrix_3, target_3))
print(fun_312(matrix_4, target_4))
print(fun_312(matrix_5, target_5))

"""<a id='3.13'></a>

## Exercise ($\star\star$)
- Problem:
    - follow up on [exercise 3.12](#3.12)
    - in this case, you can assume each row in the matrix is sorted in ascending order
    - find the solution with the complexity below
    - this exercise will be followed by
        - [exercise 3.15](#3.15)
- Complexity:
    - $O(m \times \log(n))$ time
    - $O(1)$ space
- Skills: 
    - tweaking an existing solution to fulfill the new requirements
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.13)
"""

# Implementation
def fun_313(matrix, target):
    """
    Find a target in a (m * n) matrix
    You can assume each row in the matrix is sorted in ascending order
    
    Parameters
    ----------
    matrix : list[list[int]] (i.e., a list of list of integers)
    target : an integer
    
    Returns
    ----------
    [i, j], if matrix[i, j] = target 
    None, if the target is not in the matrix
    """
    
    # Implement me
    # Corner case
    # If matrix is [] (an empty list) or [[]] (a list of an empty list)
    if len(matrix) == 0 or len(matrix[0]) == 0:
        return None

    # Get the number of rows and columns
    m, n = len(matrix), len(matrix[0])
    
    # For each row
    for i in range(m):
        # Binary search over the columns
        left, right = 0, n - 1
        while left <= right:
            # Get the middle column
            mid = left + (right - left) // 2
            
            if matrix[i][mid] == target:
                return [i, mid]
            elif matrix[i][mid] > target:
                right = mid - 1
            else:
                left = mid + 1
            
    return None

# Test
matrix_1 = []
target_1 = 2

matrix_2 = [[]]
target_2 = 2

matrix_3 = [[1], [2]]
target_3 = 2

matrix_4 = [[1,  2,  3,  4],
            [5,  6,  7,  8],
            [9, 10, 11, 12]]
target_4 = 8

matrix_5 = [[1,  2,  3,  4],
            [5,  6,  7,  8],
            [9, 10, 11, 12]]
target_5 = 13

print(fun_313(matrix_1, target_1))
print(fun_313(matrix_2, target_2))
print(fun_313(matrix_3, target_3))
print(fun_313(matrix_4, target_4))
print(fun_313(matrix_5, target_5))

"""## Discussion
The difference (in running time) between fun_312 and fun_313 can be seen in fig. 6. This illustrates the advantage of algorithms (e.g., fun_313) with log linear time complexity over ones (e.g., fun_312) with polynomial time complexity.
"""

def plot(n, funs):
    """
    Plot the run time of the functions (with respect to the input size).
    
    Parameters
    ----------
    n : an integer
    funs : a list of functions
    """
        
    x = list(range(1, n + 1))
    ys = [[] for _ in range(len(funs))]

    for i in x:
        target = np.random.randint(low=1, high=i ** 2 + 1)
        matrix = np.array([j for j in range(1, i ** 2 + 1)]).reshape(i, i)
        for j in range(len(funs)):
            start = time.time()
            funs[j](matrix, target)
            end = time.time()
            ys[j].append(end - start)
    
    for j in range(len(funs)):
        plt.plot(x, ys[j], label=funs[j].__name__)
    plt.xlabel('$n$', fontsize=20)
    plt.ylabel('Run time', fontsize=20)
    plt.xticks([min(x), max(x)], fontsize=20)
    plt.yticks([min([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))]), max([ys[j][k] for j in range(len(funs)) for k in range(len(ys[j]))])], fontsize=20)
    plt.legend(fontsize=20)
    plt.tight_layout()
    plt.show()

plot(10 ** 2, [fun_312, fun_313])
print("Figure 6. The run time of fun_312 and fun_313.")

"""<a id='3.15'></a>

## Exercise ($\star\star$)
- Problem:
    - follow up on [exercise 3.13](#3.13)
    - in this case
        - besides assuming each row in the matrix is sorted in ascending order
        - you can also assume items on row $i$ are smaller than those on row $i$ + 1
    - find the solution with the complexity below
- Complexity:
    - $O(\log(m) + \log(n))$ time
    - $O(1)$ space
- Skills: 
    - tweaking an existing solution to fulfill the new requirements
- Logic: we strongly recommend you to try to find the solution yourself before looking at the [hints](#hints_3.15)
"""

# Implementation
def fun_315(matrix, target):
    """
    Find a target in a (m * n) matrix
    You can assume each row in the matrix is sorted in ascending order
    You can also assume items on row i are smaller than those on row i + 1
    
    Parameters
    ----------
    matrix : list[list[int]] (i.e., a list of list of integers)
    target : an integer
    
    Returns
    ----------
    [i, j], if matrix[i, j] = target 
    None, if the target is not in the matrix
    """
    
    # Implement me
    # Corner case
    # If matrix is [] (an empty list) or [[]] (a list of an empty list)
    if len(matrix) == 0 or len(matrix[0]) == 0:
        return None

    # Get the number of rows and columns
    m, n = len(matrix), len(matrix[0])

    # Binary search over the rows
    left, right = 0, m - 1
    while left <= right:
        # Get the middle row
        mid = left + (right - left) // 2
        
        if matrix[mid][0] > target:
            right = mid - 1
        elif matrix[mid][n - 1] < target:
            left = mid + 1
        else:    
            # Get the row
            row = mid

            # Binary search over the columns
            left, right = 0, n - 1
            while left <= right:
                # Get the middle column
                mid = left + (right - left) // 2

                if matrix[row][mid] == target:
                    return [row, mid]
                elif matrix[row][mid] > target:
                    right = mid - 1
                else:
                    left = mid + 1
                    
            return None
            
    return None

# Test
matrix_1 = []
target_1 = 2

matrix_2 = [[]]
target_2 = 2

matrix_3 = [[1], [2]]
target_3 = 2

matrix_4 = [[1,  2,  3,  4],
            [5,  6,  7,  8],
            [9, 10, 11, 12]]
target_4 = 8

matrix_5 = [[1,  2,  3,  4],
            [5,  6,  7,  8],
            [9, 10, 11, 12]]
target_5 = 13

print(fun_315(matrix_1, target_1))
print(fun_315(matrix_2, target_2))
print(fun_315(matrix_3, target_3))
print(fun_315(matrix_4, target_4))
print(fun_315(matrix_5, target_5))

"""## Discussion
The difference (in running time) between fun_313 and fun_315 can be seen in fig. 7. This illustrates the advantage of algorithms (e.g., fun_315) with log time complexity over ones (e.g., fun_313) with log linear time complexity.
"""

plot(10 ** 2, [fun_313, fun_315])
print("Figure 7. The run time of fun_313 and fun_315.")

"""# The Master Theorem
In previous sections, we discussed the idea and implementation of binary search. We used the BST to show that its time complexity is $O(\log (n))$. Here we will introduce the *Master Theorem*, which allows us to *read* the time complexity of a recursive algorithm from the recursive definition of the complexity.

It turns out that the master theorem is closely related to the *Divide-and-Conquer* approach, where we solve a big problem by first dividing the problem into some small subproblems and then solving the small subproblems. The idea is that, solving small subproblems could be much easier than solving the big problem itself, which is the case in many scenarios.

Based on this idea, the algorithm for solving the big problem can be written as

def fun(n):

    do some work to divide the problem into subproblems
    
    for _ in range(a):
        fun(n / b)
        
    do some work to merge the result of the subproblems
    
where:
- $n$ is the size of the problem
- $a$ is the number of subproblems that need to be solved
- $n / b$ the size of each subproblem

The algorithm above says that, we divide the original problem of size $n$ into $a$ subproblems of size $n / b$. We also need to do some preparation before the division, and some summarization after solving all the subproblems. Now let $T(n)$ be the time complexity of the algorithm, and $f(n)$ the time complexity for the preparation and summarization. Then $T(n)$ can be written as:

\begin{equation}
T(n) = aT(n / b) + f(n) \quad \textrm{where} \quad a \geq 1, b > 1.\tag{4}
\end{equation}

The equation says that, the time complexity of solving the original problem, $T(n)$, equals the overall time complexity of solving all the subproblems, $aT(n / b)$, plus the time complexity for the preparation and summarization, $f(n)$.

Then depending on the relationship between $f(n)$ and $log_b a$, $T(n)$ can take one of the following three forms

\begin{equation}
T(n) = 
\begin{cases}
O \left( n^{\log_b a}\right), &\textrm{if} \quad f(n) = O (n^c) \quad \quad \quad \ \ \ \ \textrm{where} \quad c < \log_b a\tag{5} \\
O \left( n^c \log^{k + 1} n\right), &\textrm{if} \quad f(n) = O \left(n^c \log^k n \right) \quad \textrm{where} \quad c = \log_b a \quad \textrm{and} \quad k \geq 0\\
O \big( f(n) \big), &\textrm{if} \quad f(n) = O (n^c) \quad \quad \quad \ \ \ \ \textrm{where} \quad c > \log_b a\\
\end{cases}
\end{equation}

## Exercise
**Q**: Using the master theorem to find $T(n)$ which takes the following form:

\begin{equation}
T(n) = 4 T(n / 2) + n. \tag{6}
\end{equation}

**A**: Here:
- $a = 4$
- $b = 2$
- $f(n) = n^c$ where $c = 1$

Since $c = 1 < \log_b a = \log_2 4 = 2$, the first condition in eq. (5) holds. As a result we have

\begin{equation}
T(n) = O \left( n^{\log_b a}\right) = O \left( n^2\right). \tag{7}
\end{equation}

## Exercise
**Q**: Using the master theorem to find $T(n)$ which takes the following form:

\begin{equation}
T(n) = T(n / 2) + 1. \tag{8}
\end{equation}

**A**: Here:
- $a = 1$
- $b = 2$
- $f(n) = n^c \log^k n$ where $c = 0$ and $k = 0$

Since $c = 0 = \log_b a = \log_2 1 = 0$, the second condition in eq. (5) holds. As a result we have

\begin{equation}
T(n) = O \left( n^c \log^{k + 1} n\right) = O \left( \log n\right). \tag{9}
\end{equation}

There is one thing that is particularly interesting about eq. (8). It says that the algorithm divides the problem into two halves (since $b = 2$) and, more importantly, it only needs to solve one of the subproblems (since $a = 1$). Further, the time complexity for the preparation and summarization is constant (since $f(n) = 1$). Does it ring any bells?

It turns out that, the steps above are the hallmarks of binary search. In other words, the time complexity of binary search can be represented by eq. (8). Previously we used fig. (2) and eq. (3) to show that the complexity of the algorithm is $O(\log n)$, which is echoed here by the results of the master theorem.

## Exercise
**Q**: Using the master theorem to find $T(n)$ which takes the following form:

\begin{equation}
T(n) = 2 T(n / 2) + n. \tag{10}
\end{equation}

**A**: Here:
- $a = 2$
- $b = 2$
- $f(n) = n^c \log^k n$ where $c = 1$ and $k = 0$

Since $c = 1 = \log_b a = \log_2 2 = 1$, the second condition in eq. (5) holds. As a result we have

\begin{equation}
T(n) = O \left( n^c \log^{k + 1} n\right) = O \left( n \log n\right). \tag{11}
\end{equation}

Unlike eq. (8) which only has to solve one of the subproblems (since $a = 1$), eq. (10) has to solve both of the subproblems (since $a = 2$). Another difference between the two equations is that, eq. (8) uses constant time (since $f(n) = 1$) for the preparation (before dividing the original problem into subproblems) and summarization (after), whereas eq. (10) uses linear time (since $f(n) = n$) to do so. For the above two differences, the $T(n)$ is log in eq. (8) but log linear in eq. (10). We will come back to eq. (10) in greater detail in chapter 4.

## Exercise
**Q**: Using the master theorem to find $T(n)$ which takes the following form:

\begin{equation}
T(n) = 2 T(n / 2) + n^2. \tag{12}
\end{equation}

**A**: Here:
- $a = 2$
- $b = 2$
- $f(n) = n^c \log^k n$ where $c = 2$

Since $c = 2 > \log_b a = \log_2 2 = 1$, the third condition in eq. (5) holds. As a result we have

\begin{equation}
T(n) = O \left( f(n)\right) = O \left( n^2\right). \tag{13}
\end{equation}

# Appendix

<a id='hints_3.3'></a>

## Hints for [exercise 3.3](#3.3):
- the logic is almost the same as that in [example 3.1](#3.1)
- the difference is
    - when the target is found, instead of looping over the remaining items, we return its index right away (thus avoiding unnecessary computation)

<a id='hints_3.6'></a>

## Hints for [exercise 3.6](#3.6):
- the logic is almost the same as that in the binary search in [section 3.5.3](#3.5.3)
- the difference is
    - when the target is not in the input array, we return left instead of None

<a id='hints_3.7'></a>

## Hints for [exercise 3.7](#3.7):
- we loop over each number i between 0 and the target (input) and check whether the target is the square of i
- instead of checking if i * i == target, we check if i == target / i (to handle overflow)

<a id='hints_3.8'></a>

## Hints for [exercise 3.8](#3.8):
- the logic is almost the same as that in [exercise 3.6](#3.6)
- the difference is
    - instead of comparing mid with target, we compare mid with target / mid

<a id='hints_3.10'></a>

## Hints for [exercise 3.10](#3.10):
- we loop over each number i between 1 and n and return i if the helper function returns 1

<a id='hints_3.11'></a>

## Hints for [exercise 3.11](#3.11):
- the logic is almost the same as that in [exercise 3.6](#3.6)
- the difference is
    - instead of comparing mid with the target (which is hidden in this case), we check the result of the helper function

<a id='hints_3.12'></a>

## Hints for [exercise 3.12](#3.12):
- we loop over each entry [i, j] in the matrix and return [i, j] if the item in the entry is the target

<a id='hints_3.13'></a>

## Hints for [exercise 3.13](#3.13):
- the logic is similar to that in [exercise 3.12](#3.12)
- the difference is
    - in each row, instead of looping over each column (to find j), we use the binary search since items on each row are sorted

<a id='hints_3.15'></a>

## Hints for [exercise 3.15](#3.15):
- the logic is similar to that in [exercise 3.13](#3.13)
- the difference is
    - we use the binary search twice
        - first over the rows (to find i), since items on row i are smaller than those on row i + 1
        - then over the columns (to find j), since items on each row are sorted
"""

